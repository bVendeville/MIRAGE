{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBre-x3ux7uj"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dLZriWYgn6G",
    "outputId": "466f54e9-8250-495c-a4a0-e93a8d832004",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: scikit-learn in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: torch in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (3.11.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (75.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/benjamin/work/vens/mirage_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers scikit-learn pandas matplotlib spacy torch sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/benjamin/.local/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/benjamin/.local/lib/python3.12/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/benjamin/.local/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/lib/python3/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/benjamin/.local/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/benjamin/.local/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/benjamin/.local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/lib/python3/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/benjamin/.local/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/benjamin/.local/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/benjamin/.local/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/lib/python3/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /home/benjamin/.local/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/lib/python3/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install spacy --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: {sys.executable}: command not found\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SLbnB66hK4y"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "BWAGcBe_hRjY",
    "outputId": "e07d26ff-fbb8-43a5-e1ef-c71bcd3bd7af"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m full_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/datasets/final dataset/final_dataset.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m test_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/datasets/results/test/test_dataset.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mdataset\u001b[49m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/sample_data/california_housing_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "full_data_path = \"/content/drive/MyDrive/datasets/final dataset/final_dataset.jsonl\"\n",
    "test_data_path = \"/content/drive/MyDrive/datasets/results/test/test_dataset.jsonl\"\n",
    "\n",
    "del dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"/content/sample_data/california_housing_test.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pihbwIPo1eJ"
   },
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, fixed_param, **kwargs):\n",
    "        # `fixed_param` is a required parameter for the class\n",
    "        self.fixed_param = fixed_param\n",
    "\n",
    "        # Call the function from the library, passing kwargs\n",
    "        print(\"Calling some_function with the following kwargs:\")\n",
    "        some_function(**kwargs)  # Pass additional kwargs to the function\n",
    "\n",
    "# Define the external function (for illustration)\n",
    "def some_function(arg1, arg2, **kwargs):\n",
    "    print(arg1)\n",
    "    print(arg2)\n",
    "\n",
    "# Now, create an instance of the class and pass arguments\n",
    "obj = MyClass(fixed_param=\"Important\", arg1=42, arg2=\"hello\", arg3=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udVl0KyQqXMR"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from some_library import some_function\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, fixed_param, **kwargs):\n",
    "        self.fixed_param = fixed_param\n",
    "\n",
    "        # Get the signature of some_function\n",
    "        func_signature = inspect.signature(some_function)\n",
    "\n",
    "        # Extract the parameter names from the function's signature\n",
    "        valid_params = func_signature.parameters.keys()\n",
    "\n",
    "        # Filter the kwargs to only include valid ones\n",
    "        filtered_kwargs = {key: kwargs[key] for key in kwargs if key in valid_params}\n",
    "\n",
    "        # Call the function from the library, passing only filtered kwargs\n",
    "        print(f\"Calling some_function with filtered kwargs: {filtered_kwargs}\")\n",
    "        some_function(**filtered_kwargs)\n",
    "\n",
    "# Define the external function (for illustration purposes)\n",
    "def some_function(arg1=None, arg2=None):\n",
    "    print(f\"arg1 = {arg1}\")\n",
    "    print(f\"arg2 = {arg2}\")\n",
    "\n",
    "# Example usage\n",
    "obj = MyClass(fixed_param=\"Important\", arg1=42, arg2=\"hello\", arg3=\"extra\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc0dhQULQItM"
   },
   "outputs": [],
   "source": [
    "def save_dataset(dataset, save_path, save_format=\"csv\"):\n",
    "    \"\"\"\n",
    "    Save a Hugging Face dataset to the specified file format.\n",
    "\n",
    "    Args:\n",
    "    - dataset (datasets.Dataset): The dataset to save.\n",
    "    - save_path (str): The path (including filename) where the dataset will be saved.\n",
    "    - save_format (str): The format to save the dataset. Options are \"csv\", \"json\", \"parquet\", \"hf\" (default: \"csv\").\n",
    "                         \"hf\" saves the dataset in the Hugging Face native format using `save_to_disk()`.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if save_format == \"csv\":\n",
    "        dataset.to_csv(f\"{save_path}.csv\")\n",
    "        print(f\"Dataset saved as CSV at {save_path}.csv\")\n",
    "    elif save_format == \"json\":\n",
    "        dataset.to_json(f\"{save_path}.json\")\n",
    "        print(f\"Dataset saved as JSON at {save_path}.json\")\n",
    "    elif save_format == \"parquet\":\n",
    "        dataset.to_parquet(f\"{save_path}.parquet\")\n",
    "        print(f\"Dataset saved as Parquet at {save_path}.parquet\")\n",
    "    elif save_format == \"hf\":\n",
    "        dataset.save_to_disk(save_path)\n",
    "        print(f\"Dataset saved in Hugging Face format at {save_path}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported save format: {save_format}. Choose from 'csv', 'json', 'parquet', or 'hf'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjkzr_QlQJxM"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'text': [\"I love this!\", \"This is bad.\", \"Not sure about this one.\"],\n",
    "    'label': [1, 0, 1]\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Save dataset as CSV\n",
    "save_dataset(dataset, save_path=\"output/my_dataset\", save_format=\"csv\")\n",
    "\n",
    "# Save dataset as JSON\n",
    "save_dataset(dataset, save_path=\"output/my_dataset\", save_format=\"json\")\n",
    "\n",
    "# Save dataset as Parquet\n",
    "save_dataset(dataset, save_path=\"output/my_dataset\", save_format=\"parquet\")\n",
    "\n",
    "# Save dataset in Hugging Face format\n",
    "save_dataset(dataset, save_path=\"output/hf_dataset\", save_format=\"hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmf53ejOMKVA"
   },
   "source": [
    "# Notes\n",
    "TODO: Tout ce qui est noté avec \"NOTE: \" doit être enlevé à la release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eds5Xg9r7pkk"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNAYtSoW7qxB"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package: str) -> None:\n",
    "    # Check if the package is installed\n",
    "    package_spec = importlib.util.find_spec(package)\n",
    "    if package_spec is None:\n",
    "        # Package not found, install it\n",
    "        print(f\"{package} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    else:\n",
    "        print(f\"{package} is already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EOqv3wjafIe"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2MM7_zJa9nv"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "class myDataset():\n",
    "    def __init__(self, dataset_name=None, custom_data=False, dataset_type=None, local_path=None, **kwargs) -> None:\n",
    "\n",
    "        if custom_data:\n",
    "            self.dataset = load_dataset(dataset_type, data_files=test_data_path)\n",
    "        elif dataset_name:\n",
    "            self.dataset = load_dataset(dataset_name)\n",
    "        else:\n",
    "            raise ValueError(\"Provide either a dataset name or custom data\")\n",
    "\n",
    "    def save_dataset(self, path=\"\") -> None:\n",
    "        self.dataset.save_to_disk(path)\n",
    "\n",
    "    def get_dataset(self, formatted = False) -> Dataset:\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCV1UT4ZbH0_"
   },
   "outputs": [],
   "source": [
    "class bambooDataset(myDataset):\n",
    "    def __init__(self, local_path=None, **kwargs) -> None:\n",
    "        super().__init__(dataset_name = \"Bamboo\", custom_data = False, local_path = local_path)\n",
    "\n",
    "    #NOTE: Ne pas utiliser, à priori on va uploader tout déjà prêt sur huggingface.\n",
    "    def format_dataset(self) -> None :\n",
    "        dataset = getattr(self, 'dataset')\n",
    "        dataset[\"input\"] = \"Title: \" + dataset[\"title\"] + \". Content: \" + dataset[\"content\"]\n",
    "        dataset[\"output\"] = dataset[\"hypothesis\"]\n",
    "        dataset[\"reference\"] = None\n",
    "        dataset[\"label\"] = dataset[\"answer\"]\n",
    "        dataset['dataset'] = \"bamboo\"\n",
    "\n",
    "        self.formatted_dataset = dataset [[\"input\", \"output\", \"reference\", \"label\", 'dataset']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f10TeNUwahwv"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF_nMnNJ0KWt"
   },
   "source": [
    "## ScoreManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeUjYVImLem8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    log_loss, roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "class ScoreManager:\n",
    "    def __init__(self, dataset: Dataset, metric_list: list[str], on_split: str =\"test\"):\n",
    "        \"\"\"\n",
    "        Initialize the ScoreManager with dataset and metrics to compute.\n",
    "\n",
    "        Args:\n",
    "            dataset (dict): The dataset containing 'label' and 'predictions' keys.\n",
    "            metric_list (list): List of metric names to compute. Supported metrics:\n",
    "                                ['f1', 'precision', 'recall', 'accuracy',\n",
    "                                 'balanced_accuracy', 'mcc', 'kappa',\n",
    "                                 'log_loss', 'roc_values', 'auc',\n",
    "                                 'confusion_matrix', 'precision_recall_values'].\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the dataset has splits (is a DatasetDict)\n",
    "        if isinstance(dataset, DatasetDict):\n",
    "            if on_split is None:\n",
    "                raise ValueError(\"The dataset has splits, but no split was provided. Please specify a split (e.g., 'train', 'test').\")\n",
    "            # Select the provided split\n",
    "            if on_split in dataset:\n",
    "                dataset = dataset[on_split]\n",
    "            else:\n",
    "                raise ValueError(f\"Split '{on_split}' does not exist in the dataset. Available splits: {list(dataset.keys())}\")\n",
    "        # If no splits, use the entire dataset dont need to change anything\n",
    "        display(dataset)\n",
    "\n",
    "        self.labels = dataset[\"label\"]\n",
    "        self.class_scores = dataset[\"predictions\"]\n",
    "        self.predicted_labels = [self.get_predicted_label(example) for example in dataset]\n",
    "        self.metric_list = metric_list\n",
    "\n",
    "        # Initialize metrics attributes\n",
    "        self.results = {}\n",
    "        self.calculate_metrics()\n",
    "\n",
    "        # Calculate and store ROC and precision-recall results\n",
    "        #self.results['roc_auc'], self.results['roc_values'] = self.calculate_roc()\n",
    "        #self.results['pr_values'] = self.calculate_precision_recall()\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        #self.results['confusion_matrix'] = self.calculate_confusion_matrix()\n",
    "\n",
    "    def get_predicted_label(self, example):\n",
    "        \"\"\"Get the label with the highest score from predictions.\"\"\"\n",
    "        prediction_dict = example[\"predictions\"]\n",
    "        return max(prediction_dict, key=prediction_dict.get)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Calculate and store the specified metrics.\"\"\"\n",
    "        for metric in self.metric_list:\n",
    "            method_name = f\"calculate_{metric}\"\n",
    "            if hasattr(self, method_name):\n",
    "                self.results[metric] = getattr(self, method_name)()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "\n",
    "    def calculate_f1(self):\n",
    "        \"\"\"Calculate F1 score.\"\"\"\n",
    "        return f1_score(self.labels, self.predicted_labels, average='weighted')\n",
    "\n",
    "    def calculate_precision(self):\n",
    "        \"\"\"Calculate precision score.\"\"\"\n",
    "        return precision_score(self.labels, self.predicted_labels, average='weighted')\n",
    "\n",
    "    def calculate_recall(self):\n",
    "        \"\"\"Calculate recall score.\"\"\"\n",
    "        return recall_score(self.labels, self.predicted_labels, average='weighted')\n",
    "\n",
    "    def calculate_accuracy(self):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        return accuracy_score(self.labels, self.predicted_labels)\n",
    "\n",
    "    def calculate_balanced_accuracy(self):\n",
    "        \"\"\"Calculate balanced accuracy score.\"\"\"\n",
    "        return balanced_accuracy_score(self.labels, self.predicted_labels)\n",
    "\n",
    "    def calculate_mcc(self):\n",
    "        \"\"\"Calculate Matthews Correlation Coefficient.\"\"\"\n",
    "        return matthews_corrcoef(self.labels, self.predicted_labels)\n",
    "\n",
    "    def calculate_kappa(self):\n",
    "        \"\"\"Calculate Cohen's Kappa score.\"\"\"\n",
    "        return cohen_kappa_score(self.labels, self.predicted_labels)\n",
    "\n",
    "    def calculate_log_loss(self):\n",
    "        \"\"\"Calculate Log Loss (Cross-Entropy Loss).\"\"\"\n",
    "        y_pred_proba = [list(example.values()) for example in self.class_scores]\n",
    "        return log_loss(self.labels, y_pred_proba)\n",
    "\n",
    "    def calculate_roc_values(self):\n",
    "        \"\"\"Calculate ROC curve and AUC.\"\"\"\n",
    "        y_true = np.array([1 if label == 'HALL' else 0 for label in self.labels])\n",
    "        y_scores = [example['HALL'] for example in self.class_scores]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "        return {'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds}\n",
    "\n",
    "    def calculate_auc(self):\n",
    "        \"\"\"Calculate ROC curve and AUC.\"\"\"\n",
    "        y_true = np.array([1 if label == 'HALL' else 0 for label in self.labels])\n",
    "        y_scores = [example['HALL'] for example in self.class_scores]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        return roc_auc\n",
    "\n",
    "    def calculate_precision_recall_values(self):\n",
    "        \"\"\"Calculate Precision-Recall curve values.\"\"\"\n",
    "        y_true = np.array([1 if label == 'HALL' else 0 for label in self.labels])\n",
    "        y_scores = [example['HALL'] for example in self.class_scores]\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "        return {'precision': precision, 'recall': recall, 'thresholds': thresholds}\n",
    "\n",
    "    def calculate_confusion_matrix(self):\n",
    "        \"\"\"Calculate confusion matrix.\"\"\"\n",
    "        cm = confusion_matrix(self.labels, self.predicted_labels, labels=['HALL', 'NOHALL'])\n",
    "        return cm\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        \"\"\"Plot the confusion matrix.\"\"\"\n",
    "        if \"confusion_matrix\" not in self.results:\n",
    "            self.results[\"confusion_matrix\"] = self.calculate_confusion_matrix()\n",
    "\n",
    "        cm = self.results['confusion_matrix']\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(['HALL', 'NOHALL']))\n",
    "        plt.xticks(tick_marks, ['HALL', 'NOHALL'])\n",
    "        plt.yticks(tick_marks, ['HALL', 'NOHALL'])\n",
    "\n",
    "        threshold = cm.max() / 2\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            plt.text(j, i, format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "\n",
    "    def plot_metrics_bar_chart(self, metrics_to_plot):\n",
    "        \"\"\"Plot a bar chart for specified metrics.\"\"\"\n",
    "        values = [self.results[metric] for metric in metrics_to_plot if metric in self.results]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(metrics_to_plot, values, color=plt.cm.viridis(np.linspace(0, 1, len(values))))\n",
    "\n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title('Metrics Bar Chart')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 1)  # Adjust y-axis as necessary\n",
    "\n",
    "        # Adding value labels on top of the bars\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "\n",
    "    def plot_roc_curve(self):\n",
    "        \"\"\"Plot ROC curve.\"\"\"\n",
    "        if \"roc_values\" not in self.results:\n",
    "            self.results[\"roc_values\"] = self.calculate_roc_values()\n",
    "        if \"roc_auc\" not in self.results:\n",
    "            self.results[\"roc_auc\"] = self.calculate_auc()\n",
    "\n",
    "        roc_values = self.results['roc_values']\n",
    "        plt.figure()\n",
    "        plt.plot(roc_values['fpr'], roc_values['tpr'], color='blue', label='ROC curve (area = {:.2f})'.format(self.results['roc_auc']))\n",
    "        plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc='lower right')\n",
    "        return plt\n",
    "\n",
    "    def plot_precision_recall_curve(self):\n",
    "        \"\"\"Plot Precision-Recall curve.\"\"\"\n",
    "        if \"pr_values\" not in self.results:\n",
    "            self.results[\"roc_values\"] = self.calculate_precision_recall_values()\n",
    "\n",
    "        pr_values = self.results['precision_recall_values']\n",
    "        plt.figure()\n",
    "        plt.plot(pr_values['recall'], pr_values['precision'], color='blue')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        return plt\n",
    "\n",
    "    def plot(self, plot_list, metrics_bar=None, save_plots=False, output_path=None):\n",
    "        \"\"\"Plot specified metrics and visualizations from the provided list.\n",
    "\n",
    "        Args:\n",
    "            plot_list (list): List of plot types to generate.\n",
    "            metrics_bar (list, optional): Metrics to plot in the bar chart.\n",
    "            save_plots (bool, optional): Whether to save the plots to files.\n",
    "            output_path (str, optional): Path to save plots if save_plots is True.\n",
    "        \"\"\"\n",
    "        for plot_type in plot_list:\n",
    "            if plot_type == 'metrics_bar' and metrics_bar:\n",
    "                bar_plot = self.plot_metrics_bar_chart(metrics_bar)\n",
    "                if save_plots and output_path:\n",
    "                    bar_plot.savefig(f'{output_path}/metrics_bar_chart.png')\n",
    "                else:\n",
    "                    plt.show()\n",
    "            elif plot_type == 'roc_curve':\n",
    "                roc_plot = self.plot_roc_curve()\n",
    "                if save_plots and output_path:\n",
    "                    roc_plot.savefig(f'{output_path}/roc_curve.png')\n",
    "                else:\n",
    "                    plt.show()\n",
    "            elif plot_type == 'precision_recall_curve':\n",
    "                pr_plot = self.plot_precision_recall_curve()\n",
    "                if save_plots and output_path:\n",
    "                    pr_plot.savefig(f'{output_path}/precision_recall_curve.png')\n",
    "                else:\n",
    "                    plt.show()\n",
    "            elif plot_type == 'confusion_matrix':\n",
    "                cm_plot = self.plot_confusion_matrix()\n",
    "                if save_plots and output_path:\n",
    "                    cm_plot.savefig(f'{output_path}/confusion_matrix.png')\n",
    "                else:\n",
    "                    plt.show()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported plot type: {plot_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVGqa6yv0GK8"
   },
   "source": [
    "## Mymetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ys0e-e-Q6p_T"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from transformers import EvalPrediction\n",
    "import importlib.util\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "\n",
    "\n",
    "class myMetric:\n",
    "    def __init__(self, metric_name: str = None, custom_metric: str = None) -> None:\n",
    "        return None\n",
    "\n",
    "    def evaluate_dataset(self) -> None :\n",
    "        return None\n",
    "\n",
    "    def save_results(self, folder_path: str, filename: str = \"evaluated_dataset\", format: str = \"hf\", **kwargs) -> None:\n",
    "        # Create folder if it doesn't exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Automatically append the appropriate file extension based on format\n",
    "        if format == \"hf\":\n",
    "            file_path = folder_path  # No filename needed for Hugging Face format\n",
    "        elif format == \"csv\":\n",
    "            file_path = os.path.join(folder_path, f\"{filename}.csv\")\n",
    "        elif format == \"json\":\n",
    "            file_path = os.path.join(folder_path, f\"{filename}.json\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "        # Save the dataset in the specified format\n",
    "        if format == \"hf\":\n",
    "            self.evaluated_dataset.save_to_disk(folder_path)\n",
    "            print(f\"Dataset saved in Hugging Face format at {folder_path}\")\n",
    "        elif format == \"csv\":\n",
    "            self.evaluated_dataset.to_csv(file_path, **kwargs)\n",
    "            print(f\"Dataset saved as CSV at {file_path}\")\n",
    "        elif format == \"json\":\n",
    "            self.evaluated_dataset.to_json(file_path, **kwargs)\n",
    "            print(f\"Dataset saved as JSON at {file_path}\")\n",
    "\n",
    "\n",
    "    def benchmark_scores(self, metric_list: list[str] = []):\n",
    "        if not hasattr(self, \"evaluated_dataset\"):\n",
    "            print(\"no dataset to score\")\n",
    "        scores = scoreManager(dataset=self.evaluated_dataset, metric_list=[])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyGiyoB6z8cV"
   },
   "source": [
    "## transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-5DcrNPI9Fn"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedModel\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# General class for all metrics that \"just\" use a fine tuned transformer\n",
    "#  - Include defining model, loading it, providing it to user, running it on dataset\n",
    "#------------------------------------\n",
    "class transformer_model(myMetric):\n",
    "    def __init__(self, model_path: str, tokenizer_path: str, metric_name: str = \"custom\", custom_metric: bool = True) -> None:\n",
    "        super().__init__(metric_name=metric_name, custom_metric=custom_metric)\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "\n",
    "    #------------------------------------\n",
    "    # Getter, Loader, Savers\n",
    "    #------------------------------------\n",
    "    def load_model(self, save_folder: str) -> None:\n",
    "        self.model = PreTrainedModel.from_pretrained(self.model_path)\n",
    "\n",
    "\n",
    "    def load_tokenizer(self, save_folder: str) -> None:\n",
    "        self.tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    def get_model(self) -> PreTrainedModel:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_model()\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_tokenizer()\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def save_tokenizer(self, save_folder: str) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_tokenizer()\n",
    "\n",
    "        self.tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    def save_model(self, save_folder: str) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.model = self.load_model()\n",
    "\n",
    "        self.model.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    #------------------------------------\n",
    "    # Pipeline functions\n",
    "    #------------------------------------\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.pipeline = pipeline(model=self.model_path, tokenizer=self.tokenizer_path, **kwargs)\n",
    "       return self.pipeline\n",
    "\n",
    "    #Execution of the pipeline at row (or batch) level\n",
    "    #NOTE: Possibly overloaded in metric-specific class\n",
    "    def run_pipeline(self, batch, source_col, gen_col, top_k, function_to_apply, truncation, padding):\n",
    "        inputs = [[[source, gen]] for source, gen in zip(batch[source_col], batch[gen_col])]\n",
    "\n",
    "        results = self.pipeline(inputs, top_k=top_k, truncation=truncation, padding=padding, function_to_apply=function_to_apply)\n",
    "        return {\"predictions\": results}\n",
    "\n",
    "    #Execution of the pipeline on whole dataset\n",
    "    #NOTE: NOT overloaded in metric-specific class (ideally)\n",
    "    def evaluate_dataset(self,\n",
    "        dataset,\n",
    "        source_col=\"text\",\n",
    "        gen_col=\"gen\",\n",
    "        top_k=None,\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        function_to_apply=None,\n",
    "        save_result_dataset_folder_path=None,\n",
    "        save_result_dataset_format=\"hf\",\n",
    "        map_kwargs=None\n",
    "    ):\n",
    "\n",
    "        if map_kwargs is None:\n",
    "            map_kwargs = {}\n",
    "\n",
    "        #if user has not set these parameters inside kwargs then use our default params:\n",
    "        map_kwargs.setdefault(\"batched\", False)\n",
    "        map_kwargs.setdefault(\"batch_size\", 10)\n",
    "\n",
    "        #if not already created, init pipeline\n",
    "        if not hasattr(self, \"pipeline\"):\n",
    "            self.pipeline = self.create_pipeline()\n",
    "\n",
    "        self.evaluated_dataset = dataset.map(lambda batch: self.run_pipeline(batch=batch,\n",
    "                                                                             source_col=source_col,\n",
    "                                                                             gen_col=gen_col,\n",
    "                                                                             top_k=top_k,\n",
    "                                                                             truncation=truncation,\n",
    "                                                                             padding=padding,\n",
    "                                                                             function_to_apply=function_to_apply), **map_kwargs)\n",
    "\n",
    "        if save_result_dataset_folder_path:\n",
    "            self.save_results(folder_path=save_result_dataset_folder_path, format=save_result_dataset_format)\n",
    "\n",
    "        return self.evaluated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6QIOA8Pz_Tc"
   },
   "source": [
    "## TrueTeacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f_M8S876wIM"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedModel\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "\n",
    "class trueTeacher(transformer_model):\n",
    "    def __init__(self, model_path=\"google/t5_11b_trueteacher_and_anli\", tokenizer_path=\"google/t5_11b_trueteacher_and_anli\") -> None:\n",
    "        super().__init__(metric_name=\"trueTeacher\",\n",
    "                         model_path=\"google/t5_11b_trueteacher_and_anli\",\n",
    "                         tokenizer_path=\"google/t5_11b_trueteacher_and_anli\",\n",
    "                         custom_metric = False)\n",
    "\n",
    "    def load_model(self, model_path) -> None:\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_path) -> None:\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hxrujEf0BbT"
   },
   "source": [
    "## FactCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDe8d2CDkoux"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedModel\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "\n",
    "class factcc(transformer_model):\n",
    "    def __init__(self, model_path=\"manueldeprada/FactCC\", tokenizer_path=\"manueldeprada/FactCC\") -> None:\n",
    "        super().__init__(metric_name=\"factCC\", model_path=\"manueldeprada/FactCC\", tokenizer_path=\"manueldeprada/FactCC\", custom_metric = False)\n",
    "\n",
    "\n",
    "    def convert_label(self, label):\n",
    "        #We get either 'INCORRECT' or 'CORRECT'\n",
    "        #'INCORRECT' means there is a hal and 'CORRECT' means there is none\n",
    "        #   INCORRECT -> HALL\n",
    "        #   CORRECT   -> NOHALL\n",
    "        if label == 'INCORRECT':\n",
    "            return \"HALL\"\n",
    "        else:\n",
    "            return \"NOHALL\"\n",
    "\n",
    "    def format_result(self, result):\n",
    "        #Convert from:\n",
    "        #   [{'label': 'CORRECT', 'score': 0.9...},\n",
    "        #    {'label': 'INCORRECT', 'score': 0.1...}]\n",
    "        #To:\n",
    "        #   {'CORRECT': 0.9719441533088684, 'INCORRECT': 0.02805584855377674}\n",
    "\n",
    "\n",
    "        # Check if the result is batched (a list of lists) or not\n",
    "        if isinstance(result[\"predictions\"][0], list):\n",
    "            # If batched, iterate over each batch\n",
    "            return {\n",
    "                \"predictions\": [\n",
    "                    {self.convert_label(label[\"label\"]): label[\"score\"] for label in batch}\n",
    "                    for batch in result[\"predictions\"]\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            # If not batched, process directly\n",
    "            return {\n",
    "                \"predictions\": [\n",
    "                    {self.convert_label(label[\"label\"]): label[\"score\"] for label in result[\"predictions\"]}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    #Execution of the pipeline at row (or batch) level\n",
    "    # Adding a formatting\n",
    "    def run_pipeline(self, batch, source_col, gen_col, top_k, function_to_apply, truncation, padding):\n",
    "        result = super().run_pipeline(batch, source_col, gen_col, top_k, function_to_apply, truncation, padding)\n",
    "\n",
    "        return self.format_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dae08hGn0D3V"
   },
   "source": [
    "## QA based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3d9xCJZQlBu"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedModel\n",
    "from transformers import T5Tokenizer, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# General class for all metrics that use QG & QA methods\n",
    "#------------------------------------\n",
    "class qgqa_based_metric(myMetric):\n",
    "    def __init__(self,\n",
    "                 qg_model_path: str,\n",
    "                 qa_model_path: str,\n",
    "                 qg_tokenizer_path: str,\n",
    "                 qa_tokenizer_path: str = None,\n",
    "                 metric_name=\"custom\",\n",
    "                 custom_metric=True,\n",
    "                 qg_prefix=\"generate questions: \",\n",
    "                 qg_separator=\"<sep>\"):\n",
    "        super().__init__(metric_name=metric_name, custom_metric=custom_metric)\n",
    "\n",
    "        self.qg_model_path = qg_model_path  # Path to the Question Generation model\n",
    "        self.qa_model_path = qa_model_path  # Path to the Question Answering model\n",
    "        self.qg_tokenizer_path = qg_tokenizer_path  # Path to the QG Tokenizer\n",
    "        self.qa_tokenizer_path = qa_tokenizer_path or qg_tokenizer_path  # Use QG Tokenizer if QA Tokenizer not provided\n",
    "        self.qg_model = None\n",
    "        self.qa_model = None\n",
    "        self.qg_tokenizer = None\n",
    "        self.qa_tokenizer = None\n",
    "        self.qg_prefix = qg_prefix\n",
    "        self.qg_separator = qg_separator\n",
    "\n",
    "#------------------------------------\n",
    "# Getter, Loader, Savers for QG, QA, and their respective Tokenizers\n",
    "#------------------------------------\n",
    "\n",
    "    def load_qg_model(self) -> None:\n",
    "        self.qg_model = PreTrainedModel.from_pretrained(self.qg_model_path)\n",
    "\n",
    "    def load_qa_model(self) -> None:\n",
    "        self.qa_model = PreTrainedModel.from_pretrained(self.qa_model_path)\n",
    "\n",
    "    def load_qg_tokenizer(self) -> None:\n",
    "        self.qg_tokenizer = AutoTokenizer.from_pretrained(self.qg_tokenizer_path)\n",
    "\n",
    "    def load_qa_tokenizer(self) -> None:\n",
    "        self.qa_tokenizer = AutoTokenizer.from_pretrained(self.qa_tokenizer_path)\n",
    "\n",
    "    def get_qg_model(self) -> PreTrainedModel:\n",
    "        if not self.qg_model:\n",
    "            self.load_qg_model()\n",
    "        return self.qg_model\n",
    "\n",
    "    def get_qa_model(self) -> PreTrainedModel:\n",
    "        if not self.qa_model:\n",
    "            self.load_qa_model()\n",
    "        return self.qa_model\n",
    "\n",
    "    def get_qg_tokenizer(self) -> AutoTokenizer:\n",
    "        if not self.qg_tokenizer:\n",
    "            self.load_qg_tokenizer()\n",
    "        return self.qg_tokenizer\n",
    "\n",
    "    def get_qa_tokenizer(self) -> AutoTokenizer:\n",
    "        if not self.qa_tokenizer:\n",
    "            self.load_qa_tokenizer()\n",
    "        return self.qa_tokenizer\n",
    "\n",
    "    def save_qg_model(self, save_folder: str) -> None:\n",
    "        if not self.qg_model:\n",
    "            self.load_qg_model()\n",
    "        self.qg_model.save_pretrained(save_folder)\n",
    "\n",
    "    def save_qa_model(self, save_folder: str) -> None:\n",
    "        if not self.qa_model:\n",
    "            self.load_qa_model()\n",
    "        self.qa_model.save_pretrained(save_folder)\n",
    "\n",
    "    def save_qg_tokenizer(self, save_folder: str) -> None:\n",
    "        if not self.qg_tokenizer:\n",
    "            self.load_qg_tokenizer()\n",
    "        self.qg_tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "    def save_qa_tokenizer(self, save_folder: str) -> None:\n",
    "        if not self.qa_tokenizer:\n",
    "            self.load_qa_tokenizer()\n",
    "        self.qa_tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# Pipeline functions\n",
    "#------------------------------------\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_qg_pipeline(self, **kwargs) -> pipeline :\n",
    "        self.qg_pipeline = pipeline(model=self.qg_model_path, tokenizer=self.qg_tokenizer_path, truncation=True, max_length=512, **kwargs)\n",
    "        return self.qg_pipeline\n",
    "    \n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_qa_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.qa_pipeline = pipeline(model=self.qa_model_path, tokenizer=self.qa_tokenizer_path, truncation=True, max_length=512, **kwargs)\n",
    "\n",
    "       return self.qa_pipeline\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.create_qa_pipeline(**kwargs)\n",
    "       self.create_qg_pipeline(**kwargs)\n",
    "       return {\"qa_pipeline\":self.qa_pipeline,\"qg_pipeline\":self.qg_pipeline}\n",
    "\n",
    "    # Define the processing function for the question generation input\n",
    "    # Just adding the prefix\n",
    "    def format_qg_input(self, batch):\n",
    "        batch = [self.qg_prefix + text for text in batch]\n",
    "        return batch\n",
    "\n",
    "    # Define the processing function for the question generation output\n",
    "    # (mostly just to deal with the separator and empty questions)\n",
    "    # Just adding the prefix\n",
    "    def format_qg_output(self, questions):\n",
    "        questions = questions.split(self.qg_separator)\n",
    "        questions = [q for q in questions if q != \"\"]\n",
    "\n",
    "        # Remove duplicates while maintaining order\n",
    "        seen = set()\n",
    "        questions = [seen.add(item) or item for item in questions if item not in seen]\n",
    "\n",
    "        return questions\n",
    "\n",
    "    #Default function to use to get a score when comparing the answers from the source and the generation\n",
    "    def compute_token_f1(self, answers, gen_col, src_col):\n",
    "        f1_results = []\n",
    "\n",
    "        for answer in answers:\n",
    "            src_answer = answer[src_col]\n",
    "            gen_answer = answer[gen_col]\n",
    "\n",
    "            # Tokenize the answers\n",
    "            # answers generated with the qa tokenizers so we use this tokenizer here\n",
    "            tokens_src = set(self.qa_tokenizer.tokenize(src_answer))\n",
    "            tokens_gen = set(self.qa_tokenizer.tokenize(gen_answer))\n",
    "\n",
    "            # Calculate precision, recall, and F1 score\n",
    "            common_tokens = tokens_src & tokens_gen\n",
    "            precision = len(common_tokens) / len(tokens_src) if tokens_src else 0\n",
    "            recall = len(common_tokens) / len(tokens_gen) if tokens_gen else 0\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            f1_results.append(f1_score)\n",
    "\n",
    "        return f1_results\n",
    "\n",
    "    #Getting the score of a single row given the answers generated\n",
    "    def score_answers(self, answers, gen_col, src_col):\n",
    "        scores = self.compute_token_f1(answers, gen_col, src_col)\n",
    "\n",
    "        # Calculate the average score\n",
    "        if scores:  # Check if the list is not empty\n",
    "            average_score = sum(scores) / len(scores)\n",
    "        else:\n",
    "            average_score = 0\n",
    "        return average_score\n",
    "\n",
    "    #Creating and running the pipeline for generating the answer (either from generation or source, passed as context) given the question generated\n",
    "    def generate_answers(self, question, context):\n",
    "        answer = self.qa_pipeline(f\"question: {question}  context: {context}\")[0][\"generated_text\"]\n",
    "        return answer\n",
    "\n",
    "    def generate_questions(self, batch, qg_pipeline_call_args={}):\n",
    "\n",
    "        #Format to add a prefix ex: \"generate question\" and only keep the source col\n",
    "        input = self.format_qg_input(batch)\n",
    "\n",
    "        #questions = self.qg_pipeline(input, max_new_tokens=max_new_tokens, truncation=truncation)\n",
    "        questions = self.qg_pipeline(input, **qg_pipeline_call_args)\n",
    "\n",
    "        #Format the recieved questions ex:\n",
    "        #[[\"what does the fox say?<sep>who let the dogs out?<sep>Who you gonna call?\"],[\"Why did the chicken cross the road?<sep>knock knock whos there?\"]]\n",
    "        #[[\"what does the fox say?\",\"who let the dogs out?\",\"Who you gonna call?\"],[\"Why did the chicken cross the road?\",\"knock knock whos there?\"]]\n",
    "        #the comprehension is for dealing with each line separately, format_qg_output does the formatting and is the one which should be overridden if needed\n",
    "        questions = [self.format_qg_output(output[\"generated_text\"]) for output in questions]\n",
    "\n",
    "        return questions\n",
    "\n",
    "    #Execution of the pipeline at row (or batch) level\n",
    "    #NOTE: Possibly overloaded in metric-specific class\n",
    "    def run_pipeline(self, batch, batched, gen_col, src_col, qg_pipeline_call_args, keep_questions, keep_answers):\n",
    "\n",
    "        # If input is not batched it is not a dict of list\n",
    "        # Changing it to a list to homogenize rest of code\n",
    "        # Check if the input is batched (a list of items) or not\n",
    "        if not batched:\n",
    "            batch = {key: [value] for key, value in batch.items()}\n",
    "\n",
    "        #Run the question generation model on input\n",
    "        src_questions = self.generate_questions(batch[src_col], qg_pipeline_call_args)\n",
    "\n",
    "        #Run the question answering model on each question\n",
    "        #Do it in the context of the source, and of the generation\n",
    "        all_answers_batch = [\n",
    "                                [\n",
    "                                    {src_col: self.generate_answers(question, src_col),\n",
    "                                    gen_col: self.generate_answers(question, gen_col)}\n",
    "                                for question in row]\n",
    "                            for row in src_questions]\n",
    "\n",
    "        #Give a score to each answer when comparing it between contexts (source/generation) in the same line (default: token-level F1)\n",
    "        #For each lines, group the scores of all it's answers (default: Average)\n",
    "        score = [self.score_answers(answers, gen_col, src_col) for answers in all_answers_batch]\n",
    "\n",
    "\n",
    "        #If it is not batched then I need to send only a str not a list of str\n",
    "        if not batched:\n",
    "            score = score[0]\n",
    "\n",
    "        output={\"scores\": score}\n",
    "\n",
    "        if keep_questions:\n",
    "            output[\"questions\"]=src_questions\n",
    "        if keep_answers:\n",
    "            output[\"answers\"]=all_answers_batch\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n",
    "\n",
    "    #Execution of the pipeline on whole dataset\n",
    "    #NOTE: NOT overloaded in metric-specific class (ideally)\n",
    "    def evaluate_dataset(self,\n",
    "        dataset,\n",
    "        source_col=\"src\",\n",
    "        gen_col=\"text\",\n",
    "        keep_questions=False,\n",
    "        keep_answers=False,\n",
    "        top_k=None,\n",
    "        qg_pipeline_call_args={},\n",
    "        padding=False,\n",
    "        function_to_apply=None,\n",
    "        save_result_dataset_folder_path=None,\n",
    "        save_result_dataset_format=\"hf\",\n",
    "        map_kwargs=None\n",
    "    ):\n",
    "\n",
    "        if map_kwargs is None:\n",
    "            map_kwargs = {}\n",
    "\n",
    "        #if user has not set these parameters inside kwargs then use our default params:\n",
    "        map_kwargs.setdefault(\"batched\", False)\n",
    "        map_kwargs.setdefault(\"batch_size\", 10)\n",
    "\n",
    "        #if not already created, init qg & qa pipelines\n",
    "        if not hasattr(self, \"qg_pipeline\"):\n",
    "            self.pipeline = self.create_qg_pipeline()\n",
    "\n",
    "        if not hasattr(self, \"qa_pipeline\"):\n",
    "            self.pipeline = self.create_qa_pipeline()\n",
    "\n",
    "        #we will need the qa tokenizer to compute differences between answer for the source and the generation\n",
    "        #we check that it is not loaded and if not we load\n",
    "        if not self.qa_tokenizer:\n",
    "            self.load_qa_tokenizer()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.evaluated_dataset = dataset.map(lambda batch: self.run_pipeline(batch=batch,\n",
    "                                                                             src_col=source_col,\n",
    "                                                                             gen_col=gen_col,\n",
    "                                                                             qg_pipeline_call_args=qg_pipeline_call_args,\n",
    "                                                                             batched=map_kwargs[\"batched\"],\n",
    "                                                                             keep_questions=keep_questions,\n",
    "                                                                             keep_answers=keep_answers,\n",
    "                                                                             ), **map_kwargs)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if save_result_dataset_folder_path:\n",
    "            self.save_results(folder_path=save_result_dataset_folder_path, format=save_result_dataset_format)\n",
    "\n",
    "        return self.evaluated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABiThsFuvr0O"
   },
   "outputs": [],
   "source": [
    "#Reproduces the method highlited in QAGS but with other models\n",
    "\n",
    "class qags(qgqa_based_metric):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(qg_model_path=\"valhalla/t5-small-e2e-qg\",\n",
    "                        qa_model_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                        qg_tokenizer_path= \"valhalla/t5-small-e2e-qg\",\n",
    "                        qa_tokenizer_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                        metric_name=\"qags\",\n",
    "                        custom_metric=False,\n",
    "                        qg_prefix=\"generate questions: \",\n",
    "                        qg_separator=\"<sep>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9PVLES7c1n9"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Reproduces the method highlited in FEQA but with other models\n",
    "\n",
    "class feqa(qgqa_based_metric):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(qg_model_path=\"valhalla/t5-base-qg-hl\",\n",
    "                        qa_model_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                        qg_tokenizer_path= \"valhalla/t5-base-qg-hl\",\n",
    "                        qa_tokenizer_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                        metric_name=\"feqa\",\n",
    "                        custom_metric=False,\n",
    "                        qg_prefix=\"generate questions: \",\n",
    "                        qg_separator=\"<sep>\")\n",
    "\n",
    "        self.highlight_token=\"<hl>\"\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.create_qa_pipeline(**kwargs)\n",
    "       self.create_qg_pipeline(**kwargs)\n",
    "       self.spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "       return {\"qa_pipeline\":self.qa_pipeline,\"qg_pipeline\":self.qg_pipeline, \"spacy_model\":self.spacy_model}\n",
    "\n",
    "    #Used to extract all named entities and noun chunks.\n",
    "    # For each one of them we get a copy of the input where they are enclosed in the defined highlight_token (by default <hl>)\n",
    "    def spacy_entity_extraction(self, text):\n",
    "        # Process the text using spaCy\n",
    "        doc = self.spacy_model(text)\n",
    "\n",
    "        highlighted_sentences = []\n",
    "\n",
    "        # Collect all named entities and noun chunks\n",
    "        mask_targets = list(doc.ents) + list(doc.noun_chunks)\n",
    "\n",
    "        # Generate a new sentence with each target enclosed in <hl> tags\n",
    "        for target in mask_targets:\n",
    "            # Enclose the current target with <hl> tags\n",
    "            highlighted_text = text.replace(target.text, f\"{self.highlight_token}{target.text}{self.highlight_token}\")\n",
    "            highlighted_sentences.append(highlighted_text)\n",
    "\n",
    "        return highlighted_sentences\n",
    "\n",
    "    # Define the processing function for the question generation output\n",
    "    # (mostly just to deal with the separator and empty questions)\n",
    "    # Just adding the prefix\n",
    "    def format_qg_output(self, list_of_separated_questions):\n",
    "\n",
    "        #We get the answser inside the dict\n",
    "        #and we remove each question beyond the first (we assume it would only be a repetition)\n",
    "        questions = [ output[\"generated_text\"].split(self.qg_separator)[0] for output in list_of_separated_questions]\n",
    "\n",
    "        #we remove empty questions\n",
    "        questions = [q for q in questions if q != \"\"]\n",
    "\n",
    "        # Remove duplicates while maintaining order\n",
    "        seen = set()\n",
    "        questions = [seen.add(item) or item for item in questions if item not in seen]\n",
    "\n",
    "        return questions\n",
    "\n",
    "    def generate_questions(self, batch, qg_pipeline_call_args={}):\n",
    "        #Creating the masked input for each line\n",
    "        batch_of_list_of_text_with_entity_hl = [self.spacy_entity_extraction(line) for line in batch]\n",
    "\n",
    "        #Format to add a prefix ex: \"generate question\" and only keep the source col\n",
    "        formatted_batch_of_list_of_text_with_entity_hl = [self.format_qg_input(list_of_text_with_entity_hl) for list_of_text_with_entity_hl in batch_of_list_of_text_with_entity_hl]\n",
    "\n",
    "        #data in the form:\n",
    "        #[[\"generate questions: <hl>entity1.1<hl>...\", \"generate questions: ...<hl>entity1.2<hl>...\"...]\",       <- line 1 of input batch\n",
    "        # [\"generate questions: <hl>entity2.1<hl>...\", \"generate questions: ...<hl>entity2.2<hl>...\"...]\",       <- line 2 of input batch\n",
    "        #...]\n",
    "\n",
    "        #for each line of formatted_batch_of_list_of_text_with_entity_hl we want 1 question per item (entity hl)\n",
    "        #because we assume there is only a single relevant question per entity hl\n",
    "        #we generate each question\n",
    "        all_questions = [self.qg_pipeline(list_of_question, **qg_pipeline_call_args)\n",
    "                         for list_of_question in formatted_batch_of_list_of_text_with_entity_hl]\n",
    "\n",
    "        #data in the form:\n",
    "        #\n",
    "        #[[{'generated_text': 'question1.1.1<sep>question1.1.2...'},{'generated_text': 'question1.2.1<sep>question1.2.2...'},\n",
    "        #{'generated_text': 'question2.1.1<sep>question2.1.2...'},{'generated_text': 'question2.2.1<sep>question2.2.2...'},...], <-all for line 1\n",
    "        #...]\n",
    "        #\n",
    "        #   line 1 of input batch, entity 1.X-> y different questions: 1.X.y\n",
    "        #   line 2 of input batch, entity 2.X-> z different questions: 2.X.z\n",
    "        #   ...\n",
    "\n",
    "        #Format the recieved questions ex:\n",
    "        #[[\"what does the fox say?<sep>who let the dogs out?<sep>Who you gonna call?\"],[\"Why did the chicken cross the road?<sep>knock knock whos there?\"]]\n",
    "        #[[\"what does the fox say?\",\"who let the dogs out?\",\"Who you gonna call?\"],[\"Why did the chicken cross the road?\",\"knock knock whos there?\"]]\n",
    "        #the comprehension is for dealing with each line separately, format_qg_output does the formatting and is the one which should be overridden if needed\n",
    "        questions = [self.format_qg_output(questions_per_line) for questions_per_line in all_questions]\n",
    "\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zhPukikvku5"
   },
   "source": [
    "## Entity based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgfOwpmrvnee"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedModel\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# General class for all metrics that \"just\" use a fine tuned transformer\n",
    "#  - Include defining model, loading it, providing it to user, running it on dataset\n",
    "#------------------------------------\n",
    "class entity_based_metric(myMetric):\n",
    "    def __init__(self, er_model_path, er_tokenizer_path, metric_name=\"custom\", custom_metric=True) -> None:\n",
    "        super().__init__(metric_name=metric_name, custom_metric=custom_metric)\n",
    "\n",
    "        self.er_model_path = er_model_path\n",
    "        self.er_tokenizer_path = er_tokenizer_path\n",
    "\n",
    "    #------------------------------------\n",
    "    # Getter, Loader, Savers\n",
    "    #------------------------------------\n",
    "    def load_er_model(self, save_folder) -> None:\n",
    "        self.model = PreTrainedModel.from_pretrained(self.er_model_path)\n",
    "\n",
    "\n",
    "    def load_er_tokenizer(self, save_folder) -> None:\n",
    "        self.tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    def get_er_model(self) -> PreTrainedModel:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_model()\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_er_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_tokenizer()\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def save_er_tokenizer(self, save_folder) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.pipeline = self.load_tokenizer()\n",
    "\n",
    "        self.tokenizer.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    def save_er_model(self, save_folder) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.model = self.load_model()\n",
    "\n",
    "        self.model.save_pretrained(save_folder)\n",
    "\n",
    "\n",
    "    #------------------------------------\n",
    "    # Pipeline functions\n",
    "    #------------------------------------\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.create_er_pipeline(**kwargs)\n",
    "       return {\"er_pipeline\":self.er_pipeline}\n",
    "\n",
    "    #creating pipeline, not done at init to save on space if user only want to save locally or use his own pipeline\n",
    "    def create_er_pipeline(self, **kwargs) -> pipeline :\n",
    "       self.er_pipeline = pipeline(model=self.er_model_path, tokenizer=self.er_tokenizer_path, truncation=True, max_length=1024, **kwargs)\n",
    "       return self.er_pipeline\n",
    "\n",
    "    #Formatting extracted entities\n",
    "    def format_entities(self, text):\n",
    "        return text\n",
    "\n",
    "    def filter_matching_entities(self, entities1, entities2):\n",
    "        \"\"\"Filters entities from `entities1` that have matching subject and relation in `entities2`.\"\"\"\n",
    "        filtered_entities = [\n",
    "            entity1 for entity1 in entities1\n",
    "            if any(entity1[\"subject\"] == entity2[\"subject\"] and entity1[\"relation\"] == entity2[\"relation\"]\n",
    "                for entity2 in entities2)\n",
    "        ]\n",
    "        return filtered_entities\n",
    "\n",
    "    #Default method to compare entities of both gen and sources to get a score\n",
    "    def compare_entities(self, src_entities, gen_entities):\n",
    "\n",
    "        #Filter matching entities based on subject and relation\n",
    "        gen_entities_prime = self.filter_matching_entities(gen_entities, src_entities)\n",
    "\n",
    "        #Calculate proporiton of entities and relation in gen that are also present in source\n",
    "        fact_accuracy = len(gen_entities_prime) / len(gen_entities) if gen_entities else 0\n",
    "        return fact_accuracy\n",
    "\n",
    "    #Default method to compare entities of both gen and sources to get a score\n",
    "    def compare_entities(self, src_entities, gen_entities):\n",
    "\n",
    "        #Filter matching entities based on subject and relation\n",
    "        gen_entities_prime = self.filter_matching_entities(gen_entities, src_entities)\n",
    "\n",
    "        #Calculate proporiton of entities and relation in gen that are also present in source\n",
    "        fact_accuracy = len(gen_entities_prime) / len(gen_entities) if gen_entities else 0\n",
    "        return fact_accuracy\n",
    "\n",
    "    #Execution of the pipeline at row (or batch) level\n",
    "    #NOTE: Possibly overloaded in metric-specific class\n",
    "    def run_pipeline(self, batch, batched, source_col, gen_col, keep_entities, top_k, truncation, padding):\n",
    "\n",
    "        # If input is not batched it is not a dict of list\n",
    "        # Changing it to a list to homogenize rest of code\n",
    "        # Check if the input is batched (a list of items) or not\n",
    "        if not batched:\n",
    "            batch = {key: [value] for key, value in batch.items()}\n",
    "\n",
    "        #Source\n",
    "        entities_src_tokens = self.er_pipeline(batch[source_col], return_tensors=True, return_text=False)\n",
    "        entities_src_tokens = [ line[\"generated_token_ids\"] for line in entities_src_tokens]\n",
    "\n",
    "        entities_src_text = self.er_pipeline.tokenizer.batch_decode(entities_src_tokens)\n",
    "        entities_src_format = [ self.format_entities(line) for line in entities_src_text]\n",
    "\n",
    "        #Gen\n",
    "        entities_gen_tokens = self.er_pipeline(batch[gen_col], return_tensors=True, return_text=False)\n",
    "        entities_gen_tokens = [ line[\"generated_token_ids\"] for line in entities_gen_tokens]\n",
    "\n",
    "        entities_gen_text = self.er_pipeline.tokenizer.batch_decode(entities_gen_tokens)\n",
    "        entities_gen_format = [ self.format_entities(line) for line in entities_gen_text]\n",
    "\n",
    "        score = [self.compare_entities(src, gen) for src, gen in zip(entities_gen_format, entities_src_format)]\n",
    "\n",
    "        #If it is not batched then I need to send only a str not a list of str\n",
    "        if not batched:\n",
    "            score = score[0]\n",
    "\n",
    "        output={\"scores\": score}\n",
    "\n",
    "        if keep_entities:\n",
    "            if not batched:\n",
    "                entities_src_format = entities_src_format[0]\n",
    "                entities_gen_format = entities_gen_format[0]\n",
    "\n",
    "            output[\"entities_src\"]=entities_src_format\n",
    "            output[\"entities_gen\"]=entities_gen_format\n",
    "\n",
    "        return output\n",
    "\n",
    "    #Execution of the pipeline on whole dataset\n",
    "    #NOTE: NOT overloaded in metric-specific class (ideally)\n",
    "    def evaluate_dataset(self,\n",
    "        dataset,\n",
    "        source_col=\"text\",\n",
    "        gen_col=\"gen\",\n",
    "        top_k=None,\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        max_tokens_er = 1000,\n",
    "        keep_entities=False,\n",
    "        save_result_dataset_folder_path=None,\n",
    "        save_result_dataset_format=\"hf\",\n",
    "        map_kwargs=None\n",
    "    ):\n",
    "\n",
    "        if map_kwargs is None:\n",
    "            map_kwargs = {}\n",
    "        \n",
    "        #if user has not set these parameters inside kwargs then use our default params:\n",
    "        map_kwargs.setdefault(\"batched\", False)\n",
    "        map_kwargs.setdefault(\"batch_size\", 1)\n",
    "\n",
    "        #if not already created, init pipeline\n",
    "        if not hasattr(self, \"er_pipeline\"):\n",
    "            self.pipeline = self.create_er_pipeline()\n",
    "\n",
    "        with torch.no_grad():  # Ensure no gradients\n",
    "            self.evaluated_dataset = dataset.map(lambda batch: self.run_pipeline(batch=batch,\n",
    "                                                                                 batched=map_kwargs[\"batched\"],\n",
    "                                                                                 source_col=source_col,\n",
    "                                                                                 gen_col=gen_col,\n",
    "                                                                                 keep_entities=keep_entities,\n",
    "                                                                                 top_k=top_k,\n",
    "                                                                                 truncation=truncation,\n",
    "                                                                                 padding=padding), **map_kwargs)\n",
    "\n",
    "        if save_result_dataset_folder_path:\n",
    "            self.save_results(folder_path=save_result_dataset_folder_path, format=save_result_dataset_format)\n",
    "\n",
    "        return self.evaluated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ewYBlDj_GQw"
   },
   "outputs": [],
   "source": [
    "class factacc(entity_based_metric):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(er_model_path= \"Babelscape/rebel-large\",\n",
    "                         er_tokenizer_path= \"Babelscape/rebel-large\",\n",
    "                         metric_name=\"factacc\",\n",
    "                         custom_metric=False)\n",
    "\n",
    "    #Formatting extracted entities\n",
    "    def format_entities(self, text):\n",
    "        triplets = []\n",
    "        relation, subject, relation, object_ = '', '', '', ''\n",
    "        text = text.strip()\n",
    "        current = 'x'\n",
    "        for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "            if token == \"<triplet>\":\n",
    "                current = 't'\n",
    "                if relation != '':\n",
    "                    triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "                    relation = ''\n",
    "                subject = ''\n",
    "            elif token == \"<subj>\":\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "                object_ = ''\n",
    "            elif token == \"<obj>\":\n",
    "                current = 'o'\n",
    "                relation = ''\n",
    "            else:\n",
    "                if current == 't':\n",
    "                    subject += ' ' + token\n",
    "                elif current == 's':\n",
    "                    object_ += ' ' + token\n",
    "                elif current == 'o':\n",
    "                    relation += ' ' + token\n",
    "        if subject != '' and relation != '' and object_ != '':\n",
    "            triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "        return triplets\n",
    "\n",
    "\n",
    "    #Used to compare entities of both gen and sources to get a score\n",
    "    def compare_entities(self, src_entities, gen_entities):\n",
    "\n",
    "        #Filter matching entities based on subject and relation\n",
    "        src_entities_prime = self.filter_matching_entities(src_entities, gen_entities)\n",
    "        gen_entities_prime = self.filter_matching_entities(gen_entities, src_entities)\n",
    "\n",
    "        #Calculate the intersection based on matching subject, relation, and object\n",
    "        intersection = [entity for entity in src_entities_prime\n",
    "                        if any(entity[\"subject\"] == gen_entity[\"subject\"] and\n",
    "                            entity[\"relation\"] == gen_entity[\"relation\"] and\n",
    "                            entity[\"object\"] == gen_entity[\"object\"]\n",
    "                            for gen_entity in gen_entities_prime)]\n",
    "\n",
    "        #Calculate factual accuracy as precision\n",
    "        fact_accuracy = len(intersection) / len(gen_entities_prime) if gen_entities_prime else 0\n",
    "        return fact_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "df = pd.read_csv(\"../../datasets/XSUMFaith_git_repo/xsum_hallucination_annotations/hallucination_annotations_xsum_summaries.csv\")\n",
    "\n",
    "# Step 2: Keep only the required columns\n",
    "df = df[[\"system\", \"bbcid\", \"summary\", \"worker_id\", \"hallucination_type\"]]\n",
    "\n",
    "# Step 3: Group by unique summaries and aggregate annotations\n",
    "grouped_df = (\n",
    "    df.groupby([\"system\", \"bbcid\", \"summary\"])\n",
    "    .apply(lambda x: x[[\"worker_id\", \"hallucination_type\"]].to_dict(orient=\"records\"))\n",
    "    .reset_index(name=\"annotations\")\n",
    ")\n",
    "\n",
    "# Step 4: Convert to Hugging Face Dataset\n",
    "xsumfaith_annotation = Dataset.from_pandas(grouped_df)\n",
    "\n",
    "# Check the resulting dataset\n",
    "display(xsumfaith_annotation[0])  # Example of a grouped record\n",
    "display(xsumfaith_annotation.to_pandas())  # Example of a grouped record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "xsum_dataset = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Hugging Face datasets to pandas DataFrames\n",
    "df1 = xsumfaith_annotation.to_pandas()\n",
    "df2 = xsum_dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Convert the \"bbcid\" columns to the same type (e.g., both to string)\n",
    "df1['bbcid'] = df1['bbcid'].astype(str)\n",
    "df2['bbcid'] = df2['id'].astype(str)\n",
    "\n",
    "df2 = df2.drop(columns=[\"summary\"])\n",
    "\n",
    "# Perform an inner join on \"bbcid\"xsumfaith_dataset\n",
    "merged_df = pd.merge(df1, df2, on=\"bbcid\", how=\"inner\")\n",
    "merged_df = merged_df.drop(columns=[\"id\"])\n",
    "\n",
    "# Convert the merged DataFrame back to a Hugging Face Dataset\n",
    "xsumfaith_dataset = Dataset.from_pandas(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xsumfaith_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"xsumfaith_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrueTeacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Free memory on GPU 0\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Free memory on GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmetric = trueTeacher()\n",
    "\n",
    "#testmetric.create_pipeline(device=\"cuda:0\")\n",
    "#testmetric.create_pipeline(device_map=\"auto\") #Necessitate 44GB to run at full FP32\n",
    "testmetric.create_pipeline(torch_dtype=torch.float16,\n",
    "                           device_map=\"auto\",\n",
    "                           offload_folder=\"offload\",\n",
    "                           max_memory={\n",
    "                               0: \"22GB\",   # GPU 0 VRAM\n",
    "                               1: \"22GB\",   # GPU 1 VRAM\n",
    "                               \"cpu\": \"20GB\"  # Limit CPU RAM usage to 20GB\n",
    "                           }\n",
    "                          ) #Necessitate 22GB to run at FP16\n",
    "\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": False,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "\n",
    "results = testmetric.evaluate_dataset(xsumfaith_dataset,\n",
    "                                      source_col=\"document\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      truncation=True,\n",
    "                                      #qg_pipeline_call_args={\"truncation\":True, \"max_length\":1024},\n",
    "                                      save_result_dataset_folder_path=\"/home/benjamin/work/datasets/MIRAGE/results/\" + dataset_name +\"/trueTeacher/\",\n",
    "                                      map_kwargs=map_kwargs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Free memory on GPU 0\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Free memory on GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmetric = factcc()\n",
    "\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 10\n",
    "}\n",
    "\n",
    "results = testmetric.evaluate_dataset(xsumfaith_dataset,\n",
    "                                      source_col=\"document\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      truncation=True,\n",
    "                                      #qg_pipeline_call_args={\"truncation\":True, \"max_length\":1024},\n",
    "                                      save_result_dataset_folder_path=\"/home/benjamin/work/datasets/MIRAGE/results/\" + dataset_name +\"/factcc/\",\n",
    "                                      map_kwargs=map_kwargs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Free memory on GPU 0\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Free memory on GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmetric = qags()\n",
    "\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 10\n",
    "}\n",
    "\n",
    "results = testmetric.evaluate_dataset(xsumfaith_dataset,\n",
    "                                      source_col=\"document\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      qg_pipeline_call_args={\"truncation\":True, \"max_length\":512},\n",
    "                                      save_result_dataset_folder_path=\"/home/benjamin/work/datasets/MIRAGE/results/\" + dataset_name +\"/qags/\",\n",
    "                                      map_kwargs=map_kwargs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Free memory on GPU 0\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Free memory on GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmetric = feqa()\n",
    "\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 100\n",
    "}\n",
    "\n",
    "results = testmetric.evaluate_dataset(xsumfaith_dataset,\n",
    "                                      source_col=\"document\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      qg_pipeline_call_args={\"truncation\":True, \"max_length\":512},\n",
    "                                      save_result_dataset_folder_path=\"/home/benjamin/work/datasets/MIRAGE/results/\" + dataset_name +\"/feqa/\",\n",
    "                                      keep_questions=True,\n",
    "                                      keep_answers=True,\n",
    "                                      map_kwargs=map_kwargs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Free memory on GPU 0\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Free memory on GPU 1\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testmetric = factacc()\n",
    "\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 10\n",
    "}\n",
    "\n",
    "results = testmetric.evaluate_dataset(xsumfaith_dataset,\n",
    "                                      source_col=\"document\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      truncation=True,\n",
    "                                      #qg_pipeline_call_args={\"truncation\":True, \"max_length\":1024},\n",
    "                                      save_result_dataset_folder_path=\"/home/benjamin/work/datasets/MIRAGE/results/\" + dataset_name +\"/factacc/\",\n",
    "                                      map_kwargs=map_kwargs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iryKFUDzexDb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test FacAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E14Rl6zvezly"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "source1= \"Leeds showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] Leeds extended their unbeaten run against the Tigers to six matches\"\n",
    "summary1= \"Kevin Sinfield scored his first try of the season against Castleford. Leeds Rhino scored unbeaten run against Tigers to six matches. Ryan Hall was sent to Leeds Rhino for first time in his career .\"\n",
    "\n",
    "source2= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.\"\n",
    "summary2= \"Jeff Bezos announced plans to create 5,000 new jobs in Arlington, Virginia, by opening a new Amazon headquarters. The development, which will cost $25 billion, is expected to increase traffic and housing costs but will boost the economy. Amazon’s campus will use non-renewable energy.\"\n",
    "\n",
    "source3= \"Apple unveiled its latest iPhone model during a special event in Cupertino. The new iPhone 15 comes with a faster processor, improved camera capabilities, and a more durable design. The device also introduces USB-C charging, marking a shift from Apple's long-standing use of the Lightning port. Prices for the iPhone 15 start at $799, with pre-orders available from next week. CEO Tim Cook highlighted the company's focus on privacy and environmental responsibility, noting that the new phone uses recycled materials in its construction.\"\n",
    "summary3= \"Apple released the iPhone 14 with new features, including a Lightning port and improved battery life. Prices start at $999, and pre-orders begin immediately. Tim Cook emphasized the importance of faster processing speed and wireless charging.\"\n",
    "\n",
    "data = {\n",
    "    'text': [source1, source2, source3],\n",
    "    'summary': [summary1, summary2, summary3]\n",
    "}\n",
    "\n",
    "test_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# View the dataset\n",
    "print(test_dataset)\n",
    "print(test_dataset['text'])\n",
    "print(len(test_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3HAE7Sve1Jz"
   },
   "outputs": [],
   "source": [
    "#test_dataset = machintruc\n",
    "#testmetric = entity_based_metric(er_model_path= \"Babelscape/rebel-large\",\n",
    "#                                 er_tokenizer_path= \"Babelscape/rebel-large\",\n",
    "#                                 metric_name=\"custom\",\n",
    "#                                 custom_metric=True)\n",
    "testmetric = factacc()\n",
    "#testmetric.create_pipeline()\n",
    "#results = testmetric.evaluate_dataset(test_dataset)\n",
    "\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "results = testmetric.evaluate_dataset(test_dataset,\n",
    "                                      source_col=\"text\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      save_result_dataset_folder_path=\"work/datasets/MIRAGE/test_dataset\",\n",
    "                                      max_tokens_er=1000,\n",
    "                                      batched=True,\n",
    "                                      batch_size=100,\n",
    "                                      keep_entities=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC2xmRqP3owG"
   },
   "outputs": [],
   "source": [
    "display(results['entities_src'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJunmXLa8ckL"
   },
   "outputs": [],
   "source": [
    "display(results.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWPjHwz2qPVw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPJhacMBqPMs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEjwpOWr24Bn"
   },
   "outputs": [],
   "source": [
    "def filter_matching_entities(entities1, entities2):\n",
    "    \"\"\"Filters entities from `entities1` that have matching subject and relation in `entities2`.\"\"\"\n",
    "    filtered_entities = [\n",
    "        entity1 for entity1 in entities1\n",
    "        if any(entity1[\"subject\"] == entity2[\"subject\"] and entity1[\"relation\"] == entity2[\"relation\"]\n",
    "               for entity2 in entities2)\n",
    "    ]\n",
    "    return filtered_entities\n",
    "\n",
    "def calculate_factual_accuracy(src_entities, gen_entities):\n",
    "    # Step 1: Filter matching entities based on subject and relation\n",
    "    src_entities_prime = filter_matching_entities(src_entities, gen_entities)\n",
    "    gen_entities_prime = filter_matching_entities(gen_entities, src_entities)\n",
    "\n",
    "    # Step 2: Calculate the intersection based on matching subject, relation, and object\n",
    "    intersection = [entity for entity in src_entities_prime\n",
    "                    if any(entity[\"subject\"] == gen_entity[\"subject\"] and\n",
    "                           entity[\"relation\"] == gen_entity[\"relation\"] and\n",
    "                           entity[\"object\"] == gen_entity[\"object\"]\n",
    "                           for gen_entity in gen_entities_prime)]\n",
    "\n",
    "    # Step 3: Calculate factual accuracy as precision\n",
    "    fact_accuracy = len(intersection) / len(gen_entities_prime) if gen_entities_prime else 0\n",
    "    return fact_accuracy\n",
    "\n",
    "# Example usage\n",
    "src_entities = [\n",
    "    {\"subject\": \"subject1\", \"relation\": \"relation1\", \"object\": \"object1\"},\n",
    "    {\"subject\": \"subject2\", \"relation\": \"relation2\", \"object\": \"object2\"}\n",
    "]\n",
    "gen_entities = [\n",
    "    {\"subject\": \"subject1\", \"relation\": \"relation1\", \"object\": \"object1\"},\n",
    "    {\"subject\": \"subject2\", \"relation\": \"relation2\", \"object\": \"object3\"}\n",
    "]\n",
    "\n",
    "fact_accuracy = calculate_factual_accuracy(src_entities, gen_entities)\n",
    "print(\"Factual Accuracy:\", fact_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beBoySc8xS0W",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FactAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omXbtzBh5v5q",
    "outputId": "d45516cd-10e3-4735-c406-7a2f104eae3d"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "source1= \"Leeds showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] Leeds extended their unbeaten run against the Tigers to six matches\"\n",
    "summary1= \"Kevin Sinfield scored his first try of the season against Castleford. Leeds Rhino scored unbeaten run against Tigers to six matches. Ryan Hall was sent to Leeds Rhino for first time in his career .\"\n",
    "\n",
    "source2= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.\"\n",
    "summary2= \"Jeff Bezos announced plans to create 5,000 new jobs in Arlington, Virginia, by opening a new Amazon headquarters. The development, which will cost $25 billion, is expected to increase traffic and housing costs but will boost the economy. Amazon’s campus will use non-renewable energy.\"\n",
    "\n",
    "source3= \"Apple unveiled its latest iPhone model during a special event in Cupertino. The new iPhone 15 comes with a faster processor, improved camera capabilities, and a more durable design. The device also introduces USB-C charging, marking a shift from Apple's long-standing use of the Lightning port. Prices for the iPhone 15 start at $799, with pre-orders available from next week. CEO Tim Cook highlighted the company's focus on privacy and environmental responsibility, noting that the new phone uses recycled materials in its construction.\"\n",
    "summary3= \"Apple released the iPhone 14 with new features, including a Lightning port and improved battery life. Prices start at $999, and pre-orders begin immediately. Tim Cook emphasized the importance of faster processing speed and wireless charging.\"\n",
    "\n",
    "data = {\n",
    "    'text': [source1, source2, source3],\n",
    "    'summary': [summary1, summary2, summary3]\n",
    "}\n",
    "\n",
    "test_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# View the dataset\n",
    "print(test_dataset)\n",
    "print(test_dataset['text'])\n",
    "print(len(test_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9XuOV0y5v5r"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "if \"testmetric\" in locals():\n",
    "    del testmetric\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqgqrudud4hU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test relation classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkQU7bLqd-Ur"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9ooGMw0eLRJ"
   },
   "outputs": [],
   "source": [
    "# We need to use the tokenizer manually since we need special tokens.\n",
    "test_text = \"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\"\n",
    "test_text = source1\n",
    "result = triplet_extractor(test_dataset[\"text\"], return_tensors=True, return_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toHgKI_XeVew"
   },
   "outputs": [],
   "source": [
    "result = [ line[\"generated_token_ids\"] for line in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NG-tCLPuryO1"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXZ0JWyLeSfU"
   },
   "outputs": [],
   "source": [
    "extracted_text = triplet_extractor.tokenizer.batch_decode(result)\n",
    "display(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcYIcvgac-fI"
   },
   "outputs": [],
   "source": [
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'subject': subject.strip(), 'relation': relation.strip(),'object': object_.strip()})\n",
    "    return triplets\n",
    "extracted_triplets = [ extract_triplets(line) for line in extracted_text]\n",
    "display(extracted_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OmLExEZd4U6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_IYydg85hML",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [OLD]test relation classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkCvYG0ZxUc3"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"studio-ousia/luke-large-finetuned-conll-2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3cqRKS-y78e"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import LukeTokenizer, LukeForEntitySpanClassification\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5J9Pp7bxe29"
   },
   "outputs": [],
   "source": [
    "tokenizer(source1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1ENlZcb3Kgy"
   },
   "outputs": [],
   "source": [
    "text = \"Star Wars is a film written and directed by George Lucas\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "entity_spans = []\n",
    "original_word_spans = []\n",
    "for token_start in doc:\n",
    "    for token_end in doc[token_start.i:]:\n",
    "        entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n",
    "        original_word_spans.append((token_start.i, token_end.i + 1))\n",
    "\n",
    "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
    "inputs = inputs.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "max_logits, max_indices = logits[0].max(dim=1)\n",
    "\n",
    "predictions = []\n",
    "for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n",
    "    if index != 0:  # the span is not NIL\n",
    "        predictions.append((logit, span, model.config.id2label[int(index)]))\n",
    "\n",
    "# construct an IOB2 label sequence\n",
    "predicted_sequence = [\"O\"] * len(doc)\n",
    "for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n",
    "    if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n",
    "        predicted_sequence[span[0]] = \"B-\" + label\n",
    "        if span[1] - span[0] > 1:\n",
    "            predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n",
    "\n",
    "for token, label in zip(doc, predicted_sequence):\n",
    "    print(token, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZshxgcQTBqJ"
   },
   "outputs": [],
   "source": [
    "source21= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.Beyoncé lives in Los Angeles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXBH58893bGa"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-McwTjbi3gFI"
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True,)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(source21)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VS2S2fd93mcZ"
   },
   "outputs": [],
   "source": [
    "[ entity['word'] for entity in ner_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toYwgsCJCN1_"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Create a new dictionary with only the specified fields\n",
    "filtered_entity = [{key: entity[key] for key in entity.keys() if key in ['word','start','end']} for entity in ner_results]\n",
    "filtered_entity\n",
    "entity_pairs = [[entity1, entity2] for entity1, entity2 in combinations(filtered_entity, 2) if entity1['word'] != entity2['word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWNHYf_FPsdk"
   },
   "outputs": [],
   "source": [
    "def transform_entity(entity):\n",
    "    return {'word': entity['word'], 'loc': (entity['start'], entity['end'])}\n",
    "\n",
    "entity_pairs = [[transform_entity(entity) for entity in pair] for pair in entity_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkCUcoMg4sZy"
   },
   "outputs": [],
   "source": [
    "display(entity_pairs)\n",
    "len(entity_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63hI58PE_nly"
   },
   "outputs": [],
   "source": [
    "from transformers import LukeTokenizer, LukeForEntityPairClassification\n",
    "model_pair_class = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
    "tokenizer_pair_class = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQjul1rl_qsc"
   },
   "outputs": [],
   "source": [
    "text = \"Beyoncé lives in Los Angeles.\"\n",
    "entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyoncé\" and \"Los Angeles\"\n",
    "inputs = tokenizer_pair_class(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
    "outputs = model_pair_class(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class_idx = int(logits[0].argmax())\n",
    "print(\"Predicted class:\", model_pair_class.config.id2label[predicted_class_idx])\n",
    "# Predicted class: per:cities_of_residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCvqfywwNKZW"
   },
   "outputs": [],
   "source": [
    "def process_class(identity_pair, text):\n",
    "\n",
    "    inputs = tokenizer_pair_class(text, entity_spans=entity_spans, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model_pair_class(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "    print(\"Original text:\", text)\n",
    "    decoded_text = tokenizer_pair_class.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "    print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "\n",
    "    return {'object1':identity_pair[0]['word'],\n",
    "            'object2':identity_pair[1]['word'],\n",
    "            'relation':model_pair_class.config.id2label[predicted_class_idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwmQGO78VAEF"
   },
   "outputs": [],
   "source": [
    "test_identity_pair = [{'word': 'Amazon', 'loc': (0, 6)}, {'word': 'Arlington', 'loc': (57, 66)}]\n",
    "test_text = source2\n",
    "source2len = len(test_text)\n",
    "\n",
    "test_text = test_text + \"Beyoncé lives in Los Angeles. \"\n",
    "print(test_text)\n",
    "\n",
    "test_identity_pair = [{'word': 'Beyoncé', 'loc': (source2len, source2len+7)},\n",
    "                      {'word': 'Los Angeles', 'loc': (source2len+17, source2len+28)}]\n",
    "\n",
    "print(test_text[test_identity_pair[0]['loc'][0]:test_identity_pair[0]['loc'][1]])\n",
    "print(test_text[test_identity_pair[1]['loc'][0]:test_identity_pair[1]['loc'][1]])\n",
    "\n",
    "process_class(test_identity_pair, text=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HiCVo6AVq60"
   },
   "outputs": [],
   "source": [
    "test_text[567:578]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su3SC5pyMk08"
   },
   "outputs": [],
   "source": [
    "display([ process_class(identity_pair, source2) for identity_pair in entity_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyst2LCuNyGT"
   },
   "outputs": [],
   "source": [
    "source21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtUST1WSu43m"
   },
   "source": [
    "#Test FEQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UpluxVH5fI8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e8YJyTZu8c_"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpqyVdO_u7K-"
   },
   "outputs": [],
   "source": [
    "text = \"Barack Obama was born in Hawaii. He was the 44th president of the United States.\"\n",
    "text2 = \"The home was built for inspection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6vtB5SlFDjb"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_highlighted_sentences(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    highlighted_sentences = []\n",
    "\n",
    "    # Collect all named entities and noun chunks\n",
    "    mask_targets = list(doc.ents) + list(doc.noun_chunks)\n",
    "\n",
    "    # Generate a new sentence with each target enclosed in <hl> tags\n",
    "    for target in mask_targets:\n",
    "        # Enclose the current target with <hl> tags\n",
    "        highlighted_text = text.replace(target.text, f\"<hl>{target.text}</hl>\")\n",
    "        highlighted_sentences.append(highlighted_text)\n",
    "\n",
    "    return highlighted_sentences\n",
    "\n",
    "# Get separate sentences for each entity/noun phrase highlighted\n",
    "highlighted_sentences = generate_highlighted_sentences(text)\n",
    "\n",
    "# Print the results\n",
    "for sentence in highlighted_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmYM2IHyqws9"
   },
   "source": [
    "#test lib QAQG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2Cvwvh8qzRM"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "source1= \"Leeds showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] Leeds extended their unbeaten run against the Tigers to six matches\"\n",
    "summary1= \"Kevin Sinfield scored his first try of the season against Castleford. Leeds Rhino scored unbeaten run against Tigers to six matches. Ryan Hall was sent to Leeds Rhino for first time in his career .\"\n",
    "\n",
    "source2= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.\"\n",
    "summary2= \"Jeff Bezos announced plans to create 5,000 new jobs in Arlington, Virginia, by opening a new Amazon headquarters. The development, which will cost $25 billion, is expected to increase traffic and housing costs but will boost the economy. Amazon’s campus will use non-renewable energy.\"\n",
    "\n",
    "source3= \"Apple unveiled its latest iPhone model during a special event in Cupertino. The new iPhone 15 comes with a faster processor, improved camera capabilities, and a more durable design. The device also introduces USB-C charging, marking a shift from Apple's long-standing use of the Lightning port. Prices for the iPhone 15 start at $799, with pre-orders available from next week. CEO Tim Cook highlighted the company's focus on privacy and environmental responsibility, noting that the new phone uses recycled materials in its construction.\"\n",
    "summary3= \"Apple released the iPhone 14 with new features, including a Lightning port and improved battery life. Prices start at $999, and pre-orders begin immediately. Tim Cook emphasized the importance of faster processing speed and wireless charging.\"\n",
    "\n",
    "data = {\n",
    "    'text': [source1, source2, source3],\n",
    "    'summary': [summary1, summary2, summary3]\n",
    "}\n",
    "\n",
    "test_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# View the dataset\n",
    "print(test_dataset)\n",
    "print(test_dataset['text'])\n",
    "print(len(test_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfZghOj-rOU0"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "if \"testmetric\" in locals():\n",
    "    del testmetric\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38lgBUfwq4jr"
   },
   "outputs": [],
   "source": [
    "#test_dataset = machintruc\n",
    "testmetric = qgqa_based_metric(qg_model_path= \"valhalla/t5-small-e2e-qg\",\n",
    "                             qa_model_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                             qg_tokenizer_path= \"valhalla/t5-small-e2e-qg\",\n",
    "                             qa_tokenizer_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                             metric_name=\"custom\",\n",
    "                             custom_metric=True)\n",
    "testmetric = feqa()\n",
    "#testmetric.create_pipeline()\n",
    "#results = testmetric.evaluate_dataset(test_dataset)\n",
    "\n",
    "testmetric.create_pipeline(device=\"cpu\")\n",
    "results = testmetric.evaluate_dataset(test_dataset,\n",
    "                                      source_col=\"text\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      save_result_dataset_folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\",\n",
    "                                      max_tokens_qg=1000,\n",
    "                                      batched=True,\n",
    "                                      batch_size=100,\n",
    "                                      keep_answers=True,\n",
    "                                      keep_questions=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKOVbkI85rSy"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "source1= \"Leeds showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] Leeds extended their unbeaten run against the Tigers to six matches\"\n",
    "summary1= \"Kevin Sinfield scored his first try of the season against Castleford. Leeds Rhino scored unbeaten run against Tigers to six matches. Ryan Hall was sent to Leeds Rhino for first time in his career .\"\n",
    "\n",
    "source2= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.\"\n",
    "summary2= \"Jeff Bezos announced plans to create 5,000 new jobs in Arlington, Virginia, by opening a new Amazon headquarters. The development, which will cost $25 billion, is expected to increase traffic and housing costs but will boost the economy. Amazon’s campus will use non-renewable energy.\"\n",
    "\n",
    "source3= \"Apple unveiled its latest iPhone model during a special event in Cupertino. The new iPhone 15 comes with a faster processor, improved camera capabilities, and a more durable design. The device also introduces USB-C charging, marking a shift from Apple's long-standing use of the Lightning port. Prices for the iPhone 15 start at $799, with pre-orders available from next week. CEO Tim Cook highlighted the company's focus on privacy and environmental responsibility, noting that the new phone uses recycled materials in its construction.\"\n",
    "summary3= \"Apple released the iPhone 14 with new features, including a Lightning port and improved battery life. Prices start at $999, and pre-orders begin immediately. Tim Cook emphasized the importance of faster processing speed and wireless charging.\"\n",
    "\n",
    "data = {\n",
    "    'text': [source1, source2, source3],\n",
    "    'summary': [summary1, summary2, summary3]\n",
    "}\n",
    "\n",
    "test_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# View the dataset\n",
    "print(test_dataset)\n",
    "print(test_dataset['text'])\n",
    "print(len(test_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKVbQub-5rSz"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "if \"testmetric\" in locals():\n",
    "    del testmetric\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HK5jh135rS0"
   },
   "outputs": [],
   "source": [
    "#test_dataset = machintruc\n",
    "testmetric = qgqa_based_metric(qg_model_path= \"valhalla/t5-small-e2e-qg\",\n",
    "                             qa_model_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                             qg_tokenizer_path= \"valhalla/t5-small-e2e-qg\",\n",
    "                             qa_tokenizer_path= \"valhalla/t5-small-qa-qg-hl\",\n",
    "                             metric_name=\"custom\",\n",
    "                             custom_metric=True)\n",
    "testmetric = feqa()\n",
    "#testmetric.create_pipeline()\n",
    "#results = testmetric.evaluate_dataset(test_dataset)\n",
    "\n",
    "testmetric.create_pipeline(device=\"cpu\")\n",
    "results = testmetric.evaluate_dataset(test_dataset,\n",
    "                                      source_col=\"text\",\n",
    "                                      gen_col=\"summary\",\n",
    "                                      save_result_dataset_folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\",\n",
    "                                      max_tokens_qg=1000,\n",
    "                                      batched=True,\n",
    "                                      batch_size=100,\n",
    "                                      keep_answers=True,\n",
    "                                      keep_questions=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9gGxme6Yflr"
   },
   "outputs": [],
   "source": [
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLK3tzqmx4Ox"
   },
   "outputs": [],
   "source": [
    "results.to_list()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkhHSYXIdb3s"
   },
   "source": [
    "|   |                     text \t                      |                   summary \t                    |       scores          |\n",
    "|---|-------------------------------------------------|-------------------------------------------------|-----------------------|\n",
    "| 0 |Leeds showed they are in good shape to cope wi...|Kevin Sinfield scored his first try of the sea...| 0.5                \t|\n",
    "| 1 |Amazon has announced plans to open a new headq...|Jeff Bezos announced plans to create 5,000 new...| 1 \t                |\n",
    "| 2 |Apple unveiled its latest iPhone model during ...|Apple released the iPhone 14 with new features...| 1               \t    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoV66My9dW9V"
   },
   "outputs": [],
   "source": [
    "qg_model = testmetric.create_qg_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeC2MoI0ffBZ"
   },
   "outputs": [],
   "source": [
    "qg_model.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-NllCHEZzvM"
   },
   "outputs": [],
   "source": [
    "test = [{'generated_text': 'Which team extended their unbeaten run against the Tigers to six matches?'},\n",
    "{'generated_text': 'Whose retirement did Leeds show they are in good shape to cope with?'},\n",
    "{'generated_text': 'How many wins did Leeds have against Castleford?'},\n",
    "{'generated_text': 'Who did Leeds beat in the derby?'},\n",
    "{'generated_text': 'What venue hosted the derby?'},\n",
    "{'generated_text': 'Who was sent to the sin-bin for the first time in his career?'},\n",
    "{'generated_text': 'For what time of his career did Joel Moon score a try?'},\n",
    "{'generated_text': 'Who scored his first try of the season?'},\n",
    "{'generated_text': 'For what time of his career did Joel Moon score a try?'},\n",
    "{'generated_text': 'Who did Leeds extend their unbeaten run against?'},\n",
    "{'generated_text': 'How many matches did Leeds extend their unbeaten run against the Tigers to?'},\n",
    "{'generated_text': 'Which team extended their unbeaten run against the Tigers to six matches?'},\n",
    "{'generated_text': 'What did Leeds show in the derby victory over Castleford?'},\n",
    "{'generated_text': 'What did Leeds show in the defeat of Castleford?'},\n",
    "{'generated_text': 'What did Leeds show they are in good shape to cope with?'},\n",
    "{'generated_text': 'What did Leeds show in the derby victory over Castleford?'},\n",
    "{'generated_text': 'What did Leeds win against Castleford?'},\n",
    "{'generated_text': 'Who did Leeds beat in the derby?'},\n",
    "{'generated_text': 'What was the pitch of the derby?'},\n",
    "{'generated_text': 'What was the crowd at the Mend-a-Hose Jungle?'},\n",
    "{'generated_text': 'Where was the derby played?'},\n",
    "{'generated_text': 'Who was sent to the sin-bin for the first time in his career?'},\n",
    "{'generated_text': 'Where was Ryan Hall sent for the first time in his career?'},\n",
    "{'generated_text': 'For what reason was Ryan Hall sent to the sin-bin?'},\n",
    "{'generated_text': \"What was Ryan Hall's first time in the sin-bin?\"},\n",
    "{'generated_text': 'Who scored his first try of the season?'},\n",
    "{'generated_text': 'What did Joel Moon score?'},\n",
    "{'generated_text': 'What season did Joel Moon score his first try for Leeds?'},\n",
    "{'generated_text': 'Which team extended their unbeaten run against the Tigers to six matches?'},\n",
    "{'generated_text': 'What did Leeds extend to six matches against the Tigers?'},\n",
    "{'generated_text': 'Who did Leeds extend their unbeaten run against?'},\n",
    "{'generated_text': 'How many matches did Leeds extend their unbeaten run against the Tigers to?'}]\n",
    "a = [ output[\"generated_text\"].split(\"<sep>\")[0] for output in test]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niw0DcRKugI_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test QAQG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuJ74zCz3sgB"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "source1= \"Leeds showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] Leeds extended their unbeaten run against the Tigers to six matches\"\n",
    "summary1= \"Kevin Sinfield scored his first try of the season against Castleford. Leeds Rhino scored unbeaten run against Tigers to six matches. Ryan Hall was sent to Leeds Rhino for first time in his career .\"\n",
    "\n",
    "source2= \"Amazon has announced plans to open a new headquarters in Arlington, Virginia, creating over 25,000 new jobs in the area. The company has invested heavily in the region, including a $5 billion project to develop the surrounding infrastructure. Despite concerns from local residents about increased traffic and housing costs, Amazon expects the new campus to bring significant economic benefits to the region. CEO Jeff Bezos emphasized the company's commitment to sustainability, noting that the new headquarters would run entirely on renewable energy.\"\n",
    "summary2= \"Jeff Bezos announced plans to create 5,000 new jobs in Arlington, Virginia, by opening a new Amazon headquarters. The development, which will cost $25 billion, is expected to increase traffic and housing costs but will boost the economy. Amazon’s campus will use non-renewable energy.\"\n",
    "\n",
    "source3= \"Apple unveiled its latest iPhone model during a special event in Cupertino. The new iPhone 15 comes with a faster processor, improved camera capabilities, and a more durable design. The device also introduces USB-C charging, marking a shift from Apple's long-standing use of the Lightning port. Prices for the iPhone 15 start at $799, with pre-orders available from next week. CEO Tim Cook highlighted the company's focus on privacy and environmental responsibility, noting that the new phone uses recycled materials in its construction.\"\n",
    "summary3= \"Apple released the iPhone 14 with new features, including a Lightning port and improved battery life. Prices start at $999, and pre-orders begin immediately. Tim Cook emphasized the importance of faster processing speed and wireless charging.\"\n",
    "\n",
    "data = {\n",
    "    'text': [source1, source2, source3],\n",
    "    'summary': [summary1, summary2, summary3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsNzlhTezgHO"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-e2e-qg\", max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjO-DnpdzjTx"
   },
   "outputs": [],
   "source": [
    "result = pipe('generate question: [MASK] showed they are in good shape to cope with Kevin Sinfield’s retirement as they claimed a 26 - 12 derby victory over Castleford in front of a sell-out crowd at the Mend-a-Hose Jungle. [...] Ryan Hall was sent to the sin-bin for the first time in his career […] Joel Moon scored his first try of the season […] [MASK] extended their unbeaten run against the Tigers to six matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIOsLB3VMbOS"
   },
   "outputs": [],
   "source": [
    "[text[\"generated_text\"].split(\"<sep>\") for text in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlCX54Is34jR"
   },
   "outputs": [],
   "source": [
    "src_qg = pipe(\"generate questions: \" + source)[0][\"generated_text\"].split(\"<sep>\")\n",
    "if src_qg[-1] == \"\":\n",
    "    src_qg.pop()\n",
    "gen_qg = pipe(\"generate questions: \" + summary)[0][\"generated_text\"].split(\"<sep>\")\n",
    "if gen_qg[-1] == \"\":\n",
    "    gen_qg.pop()\n",
    "\n",
    "print(src_qg)\n",
    "print(gen_qg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsqwEbnZ4fxR"
   },
   "outputs": [],
   "source": [
    "src_qg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70_ONBQd6mWS"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_qa = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qa-qg-hl\", max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGL29_py9WNX"
   },
   "outputs": [],
   "source": [
    "pipe_qa(\"question: What was the name of the man who retired from Leeds?  context: \" + summary)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00YCXiBD96Hi"
   },
   "outputs": [],
   "source": [
    "answers = [{\"source\": pipe_qa(f\"question: {question}  context: {source}\")[0][\"generated_text\"],\n",
    "            \"gen\": pipe_qa(f\"question: {question}  context: {summary}\")[0][\"generated_text\"]}\n",
    "           for question in src_qg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKO2wxl6ACTd"
   },
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrvQDFHXCE07"
   },
   "outputs": [],
   "source": [
    "def compute_token_f1(answers, model_name=\"valhalla/t5-small-qa-qg-hl\"):\n",
    "    # Load tokenizer for the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    f1_results = []\n",
    "\n",
    "    for answer in answers:\n",
    "        src_answer = answer['source']\n",
    "        gen_answer = answer['gen']\n",
    "\n",
    "        # Tokenize the answers\n",
    "        tokens_src = set(tokenizer.tokenize(src_answer))\n",
    "        tokens_gen = set(tokenizer.tokenize(gen_answer))\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        common_tokens = tokens_src & tokens_gen\n",
    "        precision = len(common_tokens) / len(tokens_src) if tokens_src else 0\n",
    "        recall = len(common_tokens) / len(tokens_gen) if tokens_gen else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        f1_results.append(f1_score)\n",
    "\n",
    "    return f1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWhG8PLAAMhl"
   },
   "outputs": [],
   "source": [
    "answers_scored = compute_token_f1(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bTpmXiFDUH3"
   },
   "outputs": [],
   "source": [
    "answers_scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anYwhtW37-oU"
   },
   "source": [
    "## utilisation exemple transformermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6zTULjLM-Ip",
    "outputId": "d9e3dab1-a2e6-4bc8-de75-4f3fd8dae8a3"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "text1='''The US has \"passed the peak\" on new coronavirus cases, the White House reported. They predict that some states would reopen this month.\n",
    "The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.'''\n",
    "summary1 = '''The pandemic has almost not affected the US'''\n",
    "summary2 = '''The pandemic has not affected the US'''\n",
    "summary3 = '''The pandemic has affected the US a lot'''\n",
    "summary4 = '''The pandemic has not affected the US at all, no death at all'''\n",
    "\n",
    "\n",
    "data = {\n",
    "    'source': [text1, text1, text1, text1],\n",
    "    'text': [summary1, summary2, summary3, summary4],\n",
    "    'label': [\"HALL\", \"NOHALL\", \"NOHALL\", \"HALL\"]  # Labels corresponding to the text\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'text': [\"This is a sample text\", \"Here's another one\", \"And yet another sentence\"],\n",
    "    'label': [0, 1, 1]  # Labels corresponding to the text\n",
    "}\n",
    "\n",
    "test_dataset = Dataset.from_dict(data)\n",
    "test_dataset2 = Dataset.from_dict(data2)\n",
    "\n",
    "# View the dataset\n",
    "print(test_dataset)\n",
    "print(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "04c742e6da8c4a53a6b874191438b2f9",
      "d0f9c9957536414cac66f34cf24bb1a6",
      "f1663f2e1d1344b78f4bb9161c196fc1",
      "6400d34c517046fe850f5a59f332f0ab",
      "2108025f6d0b4c2aa383773b112a34e4",
      "b8dd69b0766b4756b80f13d31e39d976",
      "34297e061d284e5994f38d762c237b11",
      "d3144fbd3ef0485eae939b67bbab718e",
      "c5e38186e2b84e95b5fdbcad485252df",
      "31a0ec72d2034d038fdb8089fe4fb5bd",
      "c970880654a34c6b96d84571cfe9bfe0",
      "92610b3d37bf4fba804208b0f78ff6be",
      "6c2efdbc450044c8989df96f67928661",
      "2838482660e74578abc177f080f23416",
      "bf28dfd053964dcabf81d05221217667",
      "c18f20934152453c859a9d8e837284f1",
      "ad26b73ed68f43d1851e2bd3fa19e79d",
      "727dc423dd8b480b99872d25b67da798",
      "80ba8a5419f749988b09b3345c72a701",
      "c31da3187f75403fb12267c2ac10a15b",
      "9f60fda6dd5647ebbcec3f82f558bbdd",
      "6d9af4cfb3c84f769ff2f4c11f61382d"
     ]
    },
    "id": "33aoe3aIAsHU",
    "outputId": "96a981d3-6f53-45db-ac57-c8c201e28d19"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "if \"testmetric\" in locals():\n",
    "    del testmetric\n",
    "collected = gc.collect()\n",
    "\n",
    "#test_dataset = machintruc\n",
    "testmetric = factcc()\n",
    "#testmetric.create_pipeline()\n",
    "#results = testmetric.evaluate_dataset(test_dataset)\n",
    "\n",
    "testmetric.create_pipeline(device=\"cpu\")\n",
    "results = testmetric.evaluate_dataset(test_dataset, source_col=\"source\", gen_col=\"text\", save_result_dataset_folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\", truncation=\"longest_first\", padding='max_length', top_k=None, batched=False, batch_size=100  )\n",
    "\n",
    "results['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbbirruo-Uqr"
   },
   "outputs": [],
   "source": [
    "display(results.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6wIOfFY-9aw"
   },
   "outputs": [],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvGjzeQC4XT5"
   },
   "outputs": [],
   "source": [
    "pipe=pipeline(model=\"manueldeprada/FactCC\")\n",
    "input = [[[text1,summary1]],[[text1,summary3]]]\n",
    "print(input)\n",
    "pipe(input,truncation='only_first',padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cOxm2xMwG88"
   },
   "outputs": [],
   "source": [
    "results[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR46IPacAfRl"
   },
   "outputs": [],
   "source": [
    "#testmetric.save_results(folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\", format=\"hf\")\n",
    "testmetric.save_results(folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\", format=\"json\")\n",
    "testmetric.save_results(folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\", format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWMuZmqvA_kM"
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    a.labels,\n",
    "    a.class_scores,\n",
    "    a.predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YIc-34ZBQ62"
   },
   "outputs": [],
   "source": [
    "results[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bW8Vj1XL43G"
   },
   "outputs": [],
   "source": [
    "\n",
    "metrics = ['f1', 'precision', 'recall', 'accuracy', 'balanced_accuracy', 'mcc', 'kappa', 'log_loss']\n",
    "manager = ScoreManager(results, metrics)\n",
    "\n",
    "print(manager.results)  # Outputs the calculated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bhk-uBjHRlmQ"
   },
   "outputs": [],
   "source": [
    "manager.plot(['metrics_bar', 'roc_curve', 'precision_recall_curve', 'confusion_matrix'], metrics_bar=['f1', 'precision', 'recall', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCjfZ021U18P"
   },
   "outputs": [],
   "source": [
    "manager.plot(['metrics_bar', 'roc_curve', 'precision_recall_curve', 'confusion_matrix'], metrics_bar=['f1', 'precision', 'recall', 'accuracy'], save_plots=True, output_path='/content/drive/MyDrive/datasets/results/MIRAGE_Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4js-afAFx6R"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, roc_curve\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# True labels\n",
    "true_labels = a.labels\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = a.predicted_labels\n",
    "\n",
    "# Class scores: Extract the probabilities for the positive class ('NOHALL')\n",
    "positive_class_scores = [score[0]['NOHALL'] for score in a.class_scores]\n",
    "\n",
    "# Convert labels to binary (assuming 'NOHALL' is the positive class, and 'HALL' is the negative class)\n",
    "true_labels_binary = [1 if label == 'NOHALL' else 0 for label in true_labels]\n",
    "predicted_labels_binary = [1 if label == 'NOHALL' else 0 for label in predicted_labels]\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(true_labels_binary, predicted_labels_binary)\n",
    "\n",
    "# Precision and Recall\n",
    "precision = precision_score(true_labels_binary, predicted_labels_binary)\n",
    "recall = recall_score(true_labels_binary, predicted_labels_binary)\n",
    "\n",
    "# ROC AUC score\n",
    "roc_auc = roc_auc_score(true_labels_binary, positive_class_scores)\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(true_labels_binary, positive_class_scores)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(true_labels_binary, positive_class_scores)\n",
    "\n",
    "# Display the results\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "# You can also plot Precision-Recall and ROC curves if needed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=\"ROC curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jFmU9T_9hP6"
   },
   "outputs": [],
   "source": [
    "display(results.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQLx6kayXyot"
   },
   "outputs": [],
   "source": [
    "results[\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHcMsr3rfDSE"
   },
   "source": [
    "## Utilisation with benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J17Rzdo2fOKo"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files=\"/content/drive/MyDrive/datasets/results/test/test_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDWF8w4-gAre"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MAJ1BhPfHGe"
   },
   "outputs": [],
   "source": [
    "testmetric = factcc()\n",
    "testmetric.create_pipeline(device=\"cuda:0\")\n",
    "results = testmetric.evaluate_dataset(dataset, input_col=\"input\", save_result_dataset_folder_path=\"/content/drive/MyDrive/datasets/results/MIRAGE_Test/test_dataset\", truncation=\"longest_first\", padding='max_length', top_k=None, batched=True, batch_size=100  )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUu1Hy9iBec-"
   },
   "outputs": [],
   "source": [
    "results['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_wdk_IcF-Ql"
   },
   "outputs": [],
   "source": [
    "a=results['train']\n",
    "\n",
    "def get_predicted_label(example):\n",
    "    \"\"\"Get the label with the highest score from predictions.\"\"\"\n",
    "    prediction_dict = example[\"predictions\"]\n",
    "    return max(prediction_dict, key=prediction_dict.get)\n",
    "\n",
    "b = [get_predicted_label(example) for example in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1KEy99bJQOn"
   },
   "outputs": [],
   "source": [
    "def transform_labels(dataset):\n",
    "    # Define the mapping\n",
    "    label_mapping = {1: \"NOHALL\", 0: \"HALL\"}\n",
    "\n",
    "    # Update the \"label\" column\n",
    "    dataset = dataset.map(lambda x: {'label': label_mapping.get(x['label'], x['label'])})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "clean_results = transform_labels(results['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZIpgzXDC_OD"
   },
   "outputs": [],
   "source": [
    "metrics = ['f1', 'precision', 'recall', 'accuracy', 'balanced_accuracy', 'mcc', 'kappa', 'log_loss', 'roc_values', 'auc', 'confusion_matrix', 'precision_recall_values']\n",
    "manager = ScoreManager(clean_results, metrics, on_split=\"train\")\n",
    "\n",
    "\n",
    "print(manager.results)  # Outputs the calculated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSpUYZnK_yTT"
   },
   "outputs": [],
   "source": [
    "manager.plot(['metrics_bar', 'roc_curve', 'precision_recall_curve', 'confusion_matrix'], metrics_bar=['f1', 'precision', 'recall', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvkrN6AbJ-30"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrrv6ZKpkEpS"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJx3nb8wkFdx"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InAwEpzrkK8B"
   },
   "outputs": [],
   "source": [
    "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
    "!chmod +x mini.sh\n",
    "!bash ./mini.sh -b -f -p /usr/local\n",
    "!conda install -q -y jupyter\n",
    "!conda install -q -y google-colab -c conda-forge\n",
    "!python -m ipykernel install --name \"py38\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uui0MQXki0D"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwnW2pZ_lv-6"
   },
   "outputs": [],
   "source": [
    "!pip install torch===1.4.0 torchvision===0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTJOMht1s9Hj"
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset, map_kwargs=None):\n",
    "    # Initialize map_kwargs with defaults if not provided\n",
    "    if map_kwargs is None:\n",
    "        map_kwargs = {}\n",
    "\n",
    "    # Ensure some default values\n",
    "    map_kwargs.setdefault(\"batched\", False)\n",
    "    map_kwargs.setdefault(\"num_proc\", None)\n",
    "\n",
    "    # A simple processing function\n",
    "    def process(batch):\n",
    "        return {\"length\": len(batch[\"text\"])}\n",
    "\n",
    "    # Call dataset.map with unpacked map_kwargs\n",
    "    return dataset.map(process, **map_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "data = {\"text\": [\"This is a test.\", \"Another sentence.\"]}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Map arguments\n",
    "map_kwargs = {\"batched\": True}\n",
    "\n",
    "# Call the function\n",
    "result = evaluate_dataset(dataset, map_kwargs)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_function(x, y, **kwargs):\n",
    "    # Use the kwargs passed\n",
    "    print(f\"Inner x: {x}, y: {y}\")\n",
    "    print(f\"Inner kwargs: {kwargs}\")\n",
    "\n",
    "def outer_function(a, b, inner_kwargs=None):\n",
    "    # Define an inner function\n",
    "\n",
    "    # Ensure inner_kwargs is a dictionary\n",
    "    inner_kwargs = inner_kwargs or {}\n",
    "    \n",
    "    # Call the inner function, passing the unpacked inner_kwargs\n",
    "    inner_function(a + 1, b + 1, **inner_kwargs)\n",
    "\n",
    "# Call the outer function with optional inner_kwargs\n",
    "outer_function(2, 3, inner_kwargs={\"option1\": \"test\", \"option2\": 42})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "cnndm_articles = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8SLbnB66hK4y",
    "eds5Xg9r7pkk",
    "1EOqv3wjafIe",
    "ZnOvcIUhsO26",
    "EtUST1WSu43m",
    "wmYM2IHyqws9",
    "niw0DcRKugI_",
    "anYwhtW37-oU",
    "hHcMsr3rfDSE"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (mirage_venv)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04c742e6da8c4a53a6b874191438b2f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0f9c9957536414cac66f34cf24bb1a6",
       "IPY_MODEL_f1663f2e1d1344b78f4bb9161c196fc1",
       "IPY_MODEL_6400d34c517046fe850f5a59f332f0ab"
      ],
      "layout": "IPY_MODEL_2108025f6d0b4c2aa383773b112a34e4"
     }
    },
    "2108025f6d0b4c2aa383773b112a34e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2838482660e74578abc177f080f23416": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80ba8a5419f749988b09b3345c72a701",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c31da3187f75403fb12267c2ac10a15b",
      "value": 4
     }
    },
    "31a0ec72d2034d038fdb8089fe4fb5bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34297e061d284e5994f38d762c237b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6400d34c517046fe850f5a59f332f0ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31a0ec72d2034d038fdb8089fe4fb5bd",
      "placeholder": "​",
      "style": "IPY_MODEL_c970880654a34c6b96d84571cfe9bfe0",
      "value": " 4/4 [01:13&lt;00:00, 19.54s/ examples]"
     }
    },
    "6c2efdbc450044c8989df96f67928661": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad26b73ed68f43d1851e2bd3fa19e79d",
      "placeholder": "​",
      "style": "IPY_MODEL_727dc423dd8b480b99872d25b67da798",
      "value": "Saving the dataset (1/1 shards): 100%"
     }
    },
    "6d9af4cfb3c84f769ff2f4c11f61382d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "727dc423dd8b480b99872d25b67da798": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80ba8a5419f749988b09b3345c72a701": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92610b3d37bf4fba804208b0f78ff6be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c2efdbc450044c8989df96f67928661",
       "IPY_MODEL_2838482660e74578abc177f080f23416",
       "IPY_MODEL_bf28dfd053964dcabf81d05221217667"
      ],
      "layout": "IPY_MODEL_c18f20934152453c859a9d8e837284f1"
     }
    },
    "9f60fda6dd5647ebbcec3f82f558bbdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad26b73ed68f43d1851e2bd3fa19e79d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8dd69b0766b4756b80f13d31e39d976": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf28dfd053964dcabf81d05221217667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f60fda6dd5647ebbcec3f82f558bbdd",
      "placeholder": "​",
      "style": "IPY_MODEL_6d9af4cfb3c84f769ff2f4c11f61382d",
      "value": " 4/4 [00:01&lt;00:00,  2.64 examples/s]"
     }
    },
    "c18f20934152453c859a9d8e837284f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c31da3187f75403fb12267c2ac10a15b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5e38186e2b84e95b5fdbcad485252df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c970880654a34c6b96d84571cfe9bfe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0f9c9957536414cac66f34cf24bb1a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8dd69b0766b4756b80f13d31e39d976",
      "placeholder": "​",
      "style": "IPY_MODEL_34297e061d284e5994f38d762c237b11",
      "value": "Map: 100%"
     }
    },
    "d3144fbd3ef0485eae939b67bbab718e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1663f2e1d1344b78f4bb9161c196fc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3144fbd3ef0485eae939b67bbab718e",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5e38186e2b84e95b5fdbcad485252df",
      "value": 4
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
